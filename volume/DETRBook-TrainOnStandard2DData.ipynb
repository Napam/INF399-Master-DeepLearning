{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skimage import io\n",
    "\n",
    "################# DETR FUCNTIONS FOR LOSS######################## \n",
    "import sys\n",
    "sys.path.append('./detr_custom/')\n",
    "\n",
    "from models.matcher import HungarianMatcher\n",
    "from models.detr import SetCriterion\n",
    "#################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Glob\n",
    "from glob import glob\n",
    "\n",
    "from typing import Iterable, Sequence, List, Tuple, Dict, Optional, Union, Any\n",
    "from types import ModuleType\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from generators import BlenderStandardDataset, TorchStandardDataset\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "import sqlite3 as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def pytorch_init_janus_gpu():\n",
    "    device_id = 1\n",
    "    torch.cuda.set_device(device_id)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert torch.cuda.current_device() == 1, 'Using wrong GPU'\n",
    "    assert torch.cuda.device_count() == 2, 'Cannot find both GPUs'\n",
    "    assert torch.cuda.get_device_name(0) == 'GeForce RTX 2080 Ti', 'Unexpected GPU name'\n",
    "    assert torch.cuda.is_available() == True, 'GPU not available'\n",
    "    return torch.device('cuda', device_id)\n",
    "\n",
    "\n",
    "def reloader(module_or_member: Union[ModuleType, Any]):    \n",
    "    if isinstance(module_or_member, ModuleType):\n",
    "        importlib.reload(module_or_member)\n",
    "        return module\n",
    "    else:\n",
    "        module = importlib.import_module(module_or_member.__module__)\n",
    "        importlib.reload(module)\n",
    "        return module.__dict__[module_or_member.__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1 (GeForce RTX 2080 Ti)\n"
     ]
    }
   ],
   "source": [
    "seed = 42069\n",
    "seed_everything(seed)\n",
    "\n",
    "try:\n",
    "    device = pytorch_init_janus_gpu()\n",
    "    print(f'Using device: {device} ({torch.cuda.get_device_name()})')\n",
    "except AssertionError as e:\n",
    "    print('GPU could not initialize, got error:', e)\n",
    "    device = torch.device('cpu')\n",
    "    print('Device is set to CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_CACHE_DIR = 'torch_cache'\n",
    "DATASET_DIR = '/mnt/blendervol/objdet_std_data'\n",
    "SQL_TABLE = 'bboxes_std'\n",
    "BATCH_SIZE = 1\n",
    "LR = 2e-5\n",
    "NUM_CLASSES=6\n",
    "NULL_CLASS_COEF=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = db.connect(f'file:{os.path.join(DATASET_DIR,\"bboxes.db\")}?mode=ro', uri=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = pd.read_sql_query('SELECT COUNT(DISTINCT(imgnr)) FROM bboxes_std', db_con).values[0][0]\n",
    "\n",
    "TRAIN_RANGE = (0, int(3/4*n_data))\n",
    "VAL_RANGE = (int(3/4*n_data), n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir(TORCH_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        self.model = torch.hub.load(\n",
    "            os.path.join(TORCH_CACHE_DIR, 'facebookresearch_detr_master'), \n",
    "            model='detr_resnet50', \n",
    "            pretrained=True,\n",
    "            source='local'\n",
    "        )\n",
    "        \n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        \n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)\n",
    "\n",
    "model = DETRModel(NUM_CLASSES, 100)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TorchStandardDataset = reloader(TorchStandardDataset)\n",
    "dataset__ = TorchStandardDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(*VAL_RANGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, valloader, model, criterion, optimizer, n_epochs, device):\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        trainbar = tqdm(\n",
    "            iterable=enumerate(trainloader, 0), \n",
    "            total=len(trainloader), \n",
    "            unit=' batches',\n",
    "            desc=f' Epoch {epoch+1}/{n_epochs}',\n",
    "            ascii=True,\n",
    "            position=0,\n",
    "            leave=False\n",
    "        )\n",
    "\n",
    "        # Loop through batches\n",
    "        for i, (images, labels) in trainbar:\n",
    "            images = [image.to(device) for image in images]\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            output = model(images)\n",
    "        \n",
    "            loss_dict = criterion(output, labels)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        \n",
    "            # Zero parameter gradients since PyTorch will just accumulate the gradient\n",
    "            # vectors while it trains (in order to get the \"mean\" direction to move in\n",
    "            # the parameter space). Also doing it this way minimizes memory allocation\n",
    "            # etc, probably.\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            losses.backward() # Computes gradients \n",
    "            optimizer.step() # Do a gradient step\n",
    "    \n",
    "            # print statistics\n",
    "            running_train_loss += losses.item()\n",
    "            traintqdminfo = {'train loss':running_train_loss / (i+1)}\n",
    "            trainbar.set_postfix(traintqdminfo)\n",
    "         \n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        valbar = tqdm(\n",
    "            iterable=enumerate(valloader, 0), \n",
    "            total=len(trainloader), \n",
    "            unit=' batches',\n",
    "            desc=f' Validating',\n",
    "            ascii=True,\n",
    "            position=0,\n",
    "            leave=False\n",
    "        )\n",
    "        \n",
    "        # Loop through val batches\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in valbar:\n",
    "                images = [image.to(device) for image in images]\n",
    "                labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "                output = model(images)\n",
    "\n",
    "                loss_dict = criterion(output, labels)\n",
    "                weight_dict = criterion.weight_dict\n",
    "                losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "                # print statistics\n",
    "                running_val_loss += losses.item()\n",
    "                valtqdminfo = {**traintqdminfo, 'val loss':running_val_loss / (i+1)}\n",
    "                valbar.set_postfix(valtqdminfo)\n",
    "        \n",
    "        # Extra dirty tqdm hack hehe\n",
    "        trainbar.disable = False\n",
    "        trainbar.set_postfix({**traintqdminfo, **valtqdminfo})\n",
    "        trainbar.disable = True\n",
    "        print('', file=sys.stderr)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch 1/8: 100%|##########| 25/25 [00:05<00:00,  6.31 batches/s, train loss=0.416, val loss=0.453] \n",
      " Epoch 2/8: 100%|##########| 25/25 [00:05<00:00,  6.23 batches/s, train loss=0.41, val loss=0.444] \n",
      " Epoch 3/8: 100%|##########| 25/25 [00:05<00:00,  6.30 batches/s, train loss=0.405, val loss=0.446] \n",
      " Epoch 4/8: 100%|##########| 25/25 [00:05<00:00,  6.31 batches/s, train loss=0.392, val loss=0.444] \n",
      " Epoch 5/8: 100%|##########| 25/25 [00:05<00:00,  6.30 batches/s, train loss=0.382, val loss=0.443] \n",
      " Epoch 6/8: 100%|##########| 25/25 [00:05<00:00,  6.29 batches/s, train loss=0.385, val loss=0.48]  \n",
      " Epoch 7/8: 100%|##########| 25/25 [00:05<00:00,  6.27 batches/s, train loss=0.395, val loss=0.476] \n",
      " Epoch 8/8: 100%|##########| 25/25 [00:05<00:00,  6.25 batches/s, train loss=0.391, val loss=0.462] \n"
     ]
    }
   ],
   "source": [
    "weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "matcher = HungarianMatcher()\n",
    "criterion = SetCriterion(NUM_CLASSES-1, matcher, weight_dict, eos_coef = NULL_CLASS_COEF, losses=losses)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "traingen = TorchStandardDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(0,100))\n",
    "valgen = TorchStandardDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(100,200))\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    dataset=traingen,\n",
    "    batch_size=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "valloader = DataLoader(\n",
    "    dataset=valgen,\n",
    "    batch_size=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=8,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.75it/s, BABA=yeye]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "boii = tqdm(range(10), leave=False)\n",
    "\n",
    "for i in boii:\n",
    "    time.sleep(0.05)\n",
    "\n",
    "boii.disable = False\n",
    "boii.set_postfix({'BABA':'yeye'})\n",
    "# boii.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3000//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "batchnr = 4999\n",
    "n = 10000\n",
    "print(n//batch_size)\n",
    "np.arange(n)[batch_size*batchnr:batch_size*(batchnr+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train_fn(dataloader, model, criterion, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    \n",
    "    mean = np.array([0.64817397, 0.75178422, 0.43881263])\n",
    "    std = np.array([0.06833826, 0.07720845, 0.05868721])\n",
    "    \n",
    "    summary_loss = AverageMeter()\n",
    "    \n",
    "    tk0 = tqdm(dataloader, total=len(dataloader)-1)\n",
    "    \n",
    "    for step, (images, targets) in enumerate(tk0):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        loss_dict = criterion(output, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        \n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        summary_loss.update(losses.item(), BATCH_SIZE)\n",
    "        tk0.set_postfix(loss=summary_loss.avg)\n",
    "\n",
    "    return summary_loss\n",
    "\n",
    "\n",
    "def eval_fn(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    summary_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(dataloader, total=len(dataloader)-1)\n",
    "        for step, (images, targets) in enumerate(tk0):\n",
    "            \n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            output = model(images)\n",
    "        \n",
    "            loss_dict = criterion(output, targets)\n",
    "            weight_dict = criterion.weight_dict\n",
    "        \n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            \n",
    "            summary_loss.update(losses.item(),BATCH_SIZE)\n",
    "            tk0.set_postfix(loss=summary_loss.avg)\n",
    "    \n",
    "    return summary_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
