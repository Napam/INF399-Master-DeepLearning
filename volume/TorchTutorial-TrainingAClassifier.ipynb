{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_init():\n",
    "    device_id = 1\n",
    "    torch.cuda.set_device(device_id)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert torch.cuda.current_device() == 1, 'Using wrong GPU'\n",
    "    assert torch.cuda.device_count() == 2, 'Cannot find both GPUs'\n",
    "    assert torch.cuda.get_device_name(0) == 'GeForce RTX 2080 Ti', 'Wrong GPU name'\n",
    "    assert torch.cuda.is_available() == True, 'GPU not available'\n",
    "    return torch.device('cuda', device_id)\n",
    "    \n",
    "device = pytorch_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a dataset class that is compatible with the torch.utils.data.DataLoader thing you should subclass **VisionDataset** from *torchvision.datasets.vision.VisionDataset* and implement *__getitem__* and *__len__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Returns a Dataset object\n",
    "trainset = torchvision.datasets.CIFAR10(root='data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "# Equivalent to Tensorflow generators, \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=20)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=20)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img, labels: list):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images), str(labels.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # in_channels: Input channels (3 since RGB)\n",
    "        # out_channels: Number of filters basically\n",
    "        # kernel_size: u know what it is\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        # TODO: Figure out why 16*5*5\n",
    "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Like reshape\n",
    "        x = x.view(-1 , 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Using nn.CrossEntropyLoss will kinda implicaitly apply some kind \n",
    "        # of softmax or something idk\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# .to method can be use to cast to many different things, and is used to \"cast\" tensors to GPU \n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(images.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/60: 100%|##########| 391/391 [00:01<00:00, 273.62 batches/s, loss=2.3]\n",
      " Epoch 2/60: 100%|##########| 391/391 [00:01<00:00, 272.80 batches/s, loss=2.29]\n",
      " Epoch 3/60: 100%|##########| 391/391 [00:01<00:00, 275.52 batches/s, loss=2.19]\n",
      " Epoch 4/60: 100%|##########| 391/391 [00:01<00:00, 272.03 batches/s, loss=2.02]\n",
      " Epoch 5/60: 100%|##########| 391/391 [00:01<00:00, 273.72 batches/s, loss=1.91]\n",
      " Epoch 6/60: 100%|##########| 391/391 [00:01<00:00, 273.99 batches/s, loss=1.79]\n",
      " Epoch 7/60: 100%|##########| 391/391 [00:01<00:00, 276.45 batches/s, loss=1.68]\n",
      " Epoch 8/60: 100%|##########| 391/391 [00:01<00:00, 270.59 batches/s, loss=1.62]\n",
      " Epoch 9/60: 100%|##########| 391/391 [00:01<00:00, 273.32 batches/s, loss=1.57]\n",
      " Epoch 10/60: 100%|##########| 391/391 [00:01<00:00, 278.32 batches/s, loss=1.52]\n",
      " Epoch 11/60: 100%|##########| 391/391 [00:01<00:00, 275.11 batches/s, loss=1.48]\n",
      " Epoch 12/60: 100%|##########| 391/391 [00:01<00:00, 276.45 batches/s, loss=1.44]\n",
      " Epoch 13/60: 100%|##########| 391/391 [00:01<00:00, 278.71 batches/s, loss=1.41]\n",
      " Epoch 14/60: 100%|##########| 391/391 [00:01<00:00, 278.27 batches/s, loss=1.39]\n",
      " Epoch 15/60: 100%|##########| 391/391 [00:01<00:00, 273.69 batches/s, loss=1.37]\n",
      " Epoch 16/60: 100%|##########| 391/391 [00:01<00:00, 273.68 batches/s, loss=1.34]\n",
      " Epoch 17/60: 100%|##########| 391/391 [00:01<00:00, 275.62 batches/s, loss=1.33]\n",
      " Epoch 18/60: 100%|##########| 391/391 [00:01<00:00, 281.42 batches/s, loss=1.3] \n",
      " Epoch 19/60: 100%|##########| 391/391 [00:01<00:00, 274.97 batches/s, loss=1.28]\n",
      " Epoch 20/60: 100%|##########| 391/391 [00:01<00:00, 270.42 batches/s, loss=1.26]\n",
      " Epoch 21/60: 100%|##########| 391/391 [00:01<00:00, 274.38 batches/s, loss=1.25]\n",
      " Epoch 22/60: 100%|##########| 391/391 [00:01<00:00, 271.72 batches/s, loss=1.23]\n",
      " Epoch 23/60: 100%|##########| 391/391 [00:01<00:00, 273.87 batches/s, loss=1.21]\n",
      " Epoch 24/60: 100%|##########| 391/391 [00:01<00:00, 277.35 batches/s, loss=1.2] \n",
      " Epoch 25/60: 100%|##########| 391/391 [00:01<00:00, 270.54 batches/s, loss=1.18]\n",
      " Epoch 26/60: 100%|##########| 391/391 [00:01<00:00, 275.64 batches/s, loss=1.17]\n",
      " Epoch 27/60: 100%|##########| 391/391 [00:01<00:00, 272.64 batches/s, loss=1.15]\n",
      " Epoch 28/60: 100%|##########| 391/391 [00:01<00:00, 273.23 batches/s, loss=1.14]\n",
      " Epoch 29/60: 100%|##########| 391/391 [00:01<00:00, 278.67 batches/s, loss=1.12]\n",
      " Epoch 30/60: 100%|##########| 391/391 [00:01<00:00, 278.40 batches/s, loss=1.11]\n",
      " Epoch 31/60: 100%|##########| 391/391 [00:01<00:00, 273.93 batches/s, loss=1.11]\n",
      " Epoch 32/60: 100%|##########| 391/391 [00:01<00:00, 279.41 batches/s, loss=1.09]\n",
      " Epoch 33/60: 100%|##########| 391/391 [00:01<00:00, 270.77 batches/s, loss=1.08]\n",
      " Epoch 34/60: 100%|##########| 391/391 [00:01<00:00, 270.99 batches/s, loss=1.07]\n",
      " Epoch 35/60: 100%|##########| 391/391 [00:01<00:00, 270.54 batches/s, loss=1.06]\n",
      " Epoch 36/60: 100%|##########| 391/391 [00:01<00:00, 270.24 batches/s, loss=1.05]\n",
      " Epoch 37/60: 100%|##########| 391/391 [00:01<00:00, 274.46 batches/s, loss=1.04]\n",
      " Epoch 38/60: 100%|##########| 391/391 [00:01<00:00, 275.26 batches/s, loss=1.03]\n",
      " Epoch 39/60: 100%|##########| 391/391 [00:01<00:00, 274.71 batches/s, loss=1.02]\n",
      " Epoch 40/60: 100%|##########| 391/391 [00:01<00:00, 275.89 batches/s, loss=1.01]\n",
      " Epoch 41/60: 100%|##########| 391/391 [00:01<00:00, 280.00 batches/s, loss=1.01] \n",
      " Epoch 42/60: 100%|##########| 391/391 [00:01<00:00, 273.64 batches/s, loss=0.994]\n",
      " Epoch 43/60: 100%|##########| 391/391 [00:01<00:00, 275.23 batches/s, loss=0.989]\n",
      " Epoch 44/60: 100%|##########| 391/391 [00:01<00:00, 276.26 batches/s, loss=0.978]\n",
      " Epoch 45/60: 100%|##########| 391/391 [00:01<00:00, 275.72 batches/s, loss=0.969]\n",
      " Epoch 46/60: 100%|##########| 391/391 [00:01<00:00, 270.63 batches/s, loss=0.958]\n",
      " Epoch 47/60: 100%|##########| 391/391 [00:01<00:00, 274.08 batches/s, loss=0.953]\n",
      " Epoch 48/60: 100%|##########| 391/391 [00:01<00:00, 270.96 batches/s, loss=0.944]\n",
      " Epoch 49/60: 100%|##########| 391/391 [00:01<00:00, 277.98 batches/s, loss=0.936]\n",
      " Epoch 50/60: 100%|##########| 391/391 [00:01<00:00, 273.07 batches/s, loss=0.926]\n",
      " Epoch 51/60: 100%|##########| 391/391 [00:01<00:00, 274.03 batches/s, loss=0.919]\n",
      " Epoch 52/60: 100%|##########| 391/391 [00:01<00:00, 278.88 batches/s, loss=0.913]\n",
      " Epoch 53/60: 100%|##########| 391/391 [00:01<00:00, 279.93 batches/s, loss=0.907]\n",
      " Epoch 54/60: 100%|##########| 391/391 [00:01<00:00, 274.10 batches/s, loss=0.897]\n",
      " Epoch 55/60: 100%|##########| 391/391 [00:01<00:00, 271.71 batches/s, loss=0.89] \n",
      " Epoch 56/60: 100%|##########| 391/391 [00:01<00:00, 270.96 batches/s, loss=0.88] \n",
      " Epoch 57/60: 100%|##########| 391/391 [00:01<00:00, 270.64 batches/s, loss=0.873]\n",
      " Epoch 58/60: 100%|##########| 391/391 [00:01<00:00, 274.46 batches/s, loss=0.864]\n",
      " Epoch 59/60: 100%|##########| 391/391 [00:01<00:00, 271.66 batches/s, loss=0.856]\n",
      " Epoch 60/60: 100%|##########| 391/391 [00:01<00:00, 272.10 batches/s, loss=0.849]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 60\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(\n",
    "        iterable=enumerate(trainloader, 0), \n",
    "        total=len(trainloader), \n",
    "        unit=' batches',\n",
    "        desc=f' Epoch {epoch+1}/{n_epochs}',\n",
    "        file=sys.stdout,\n",
    "        ascii=True,\n",
    "        position=0\n",
    "    )\n",
    "    \n",
    "    # Loop through batches\n",
    "    for i, data in bar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                \n",
    "        # Zero parameter gradients since PyTorch will just accumulate the gradient\n",
    "        # vectors while it trains (in order to get the \"mean\" direction to move in\n",
    "        # the parameter space). Also doing it this way minimizes memory allocation \n",
    "        # etc, probably. \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # Computes gradients \n",
    "        optimizer.step() # Do a gradient step\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        bar.set_postfix({'loss':running_loss / (i+1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'models/cifar_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load('models/cifar_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "62"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
