{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skimage import io\n",
    "\n",
    "################# DETR FUCNTIONS FOR LOSS #######################\n",
    "import sys\n",
    "sys.path.append('./detr_custom/')\n",
    "\n",
    "from models.matcher import HungarianMatcher\n",
    "from models.detr import SetCriterion\n",
    "import engine\n",
    "#################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Glob\n",
    "from glob import glob\n",
    "\n",
    "from typing import Iterable, Sequence, List, Tuple, Dict, Optional, Union, Any, Callable, Mapping\n",
    "from types import ModuleType\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from generators import BlenderStandardDataset, TorchStandardDataset, TorchStereoDataset\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "import sqlite3 as db\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def pytorch_init_janus_gpu():\n",
    "    device_id = 1\n",
    "    torch.cuda.set_device(device_id)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert torch.cuda.current_device() == 1, 'Using wrong GPU'\n",
    "    assert torch.cuda.device_count() == 2, 'Cannot find both GPUs'\n",
    "    assert torch.cuda.get_device_name(0) == 'GeForce RTX 2080 Ti', 'Unexpected GPU name'\n",
    "    assert torch.cuda.is_available() == True, 'GPU not available'\n",
    "    return torch.device('cuda', device_id)\n",
    "\n",
    "\n",
    "def reloader(module_or_member: Union[ModuleType, Any]):    \n",
    "    if isinstance(module_or_member, ModuleType):\n",
    "        importlib.reload(module_or_member)\n",
    "        return module\n",
    "    else:\n",
    "        module = importlib.import_module(module_or_member.__module__)\n",
    "        importlib.reload(module)\n",
    "        return module.__dict__[module_or_member.__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name('cuda:1'))\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1 (GeForce RTX 2080 Ti)\n"
     ]
    }
   ],
   "source": [
    "seed = 42069\n",
    "seed_everything(seed)\n",
    "\n",
    "try:\n",
    "    device = pytorch_init_janus_gpu()\n",
    "    print(f'Using device: {device} ({torch.cuda.get_device_name()})')\n",
    "except AssertionError as e:\n",
    "    print('GPU could not initialize, got error:', e)\n",
    "    device = torch.device('cpu')\n",
    "    print('Device is set to CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_CACHE_DIR = 'torch_cache'\n",
    "DATASET_DIR = '/mnt/blendervol/objdet_std_data'\n",
    "DATASET_DIR = '/mnt/blendervol/leftright_left_data'\n",
    "SQL_TABLE = 'bboxes_std'\n",
    "WEIGHTS_DIR = 'fish_statedicts'\n",
    "NUM_CLASSES=6+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = db.connect(f'file:{os.path.join(DATASET_DIR,\"bboxes.db\")}?mode=ro', uri=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2name = eval(open(os.path.join(DATASET_DIR,\"metadata.txt\"), 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = pd.read_sql_query('SELECT COUNT(DISTINCT(imgnr)) FROM bboxes_std', db_con).values[0][0]\n",
    "\n",
    "TRAIN_RANGE = (0, int(3/4*n_data))\n",
    "VAL_RANGE = (int(3/4*n_data), n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir(TORCH_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  \n",
    "class DETRModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        self.model = torch.hub.load(\n",
    "            os.path.join(TORCH_CACHE_DIR, 'facebookresearch_detr_master'), \n",
    "            model='detr_resnet50', \n",
    "            pretrained=True,\n",
    "            source='local'\n",
    "        )\n",
    "        \n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        \n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)\n",
    "\n",
    "model = DETRModel(NUM_CLASSES, 100)\n",
    "model = model.to(device)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(obj: Any, f: str):\n",
    "    pathlib.Path(f).parent.mkdir(parents=True, exist_ok=True)\n",
    "    assert isinstance(f, str), \"Filename must be of type string when saving model\"\n",
    "    torch.save(obj=obj, f=f)\n",
    "    \n",
    "    \n",
    "def interruptable(f: Callable):\n",
    "    '''Decorator for functions that should handle KeyboardInterrupts more gracefully'''\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            retval = f(*args, **kwargs)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print('Process interrupted')\n",
    "            return \n",
    "        return retval\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def img_handler(images: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    # pp for pre process\n",
    "    pp = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.Normalize([0.485, 0.456, 0.406], \n",
    "                    [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return [\n",
    "        (pp(pair[0]).to(device), pp(pair[1]).to(device)) \n",
    "        for pair in images\n",
    "    ]\n",
    "    \n",
    "    \n",
    "def label_handler(labels):\n",
    "    return [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "    \n",
    "def _validate_model(context, traintqdminfo):\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # valbar will disappear after it is done since leave=False\n",
    "    valbar = tqdm(\n",
    "        iterable=enumerate(context['valloader'], 0), \n",
    "        total=len(context['valloader']), \n",
    "        unit=' batches',\n",
    "        desc=f' Validating',\n",
    "        ascii=True,\n",
    "        position=0,\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    model = context['model']\n",
    "    criterion = context['criterion']\n",
    "    \n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    # Loop through val batches\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in valbar:\n",
    "            # images = [image.to(device) for image in images]\n",
    "            images = img_handler(images)\n",
    "            labels = label_handler(labels)\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            loss_dict = criterion(output, labels)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "            # print statistics\n",
    "            running_val_loss += losses.item()\n",
    "            val_loss = running_val_loss / (i+1)\n",
    "            valtqdminfo = {**traintqdminfo, 'val loss':val_loss}\n",
    "            valbar.set_postfix(valtqdminfo)\n",
    "            \n",
    "    return valtqdminfo\n",
    "\n",
    "\n",
    "def _train_model(context, epoch, n_epochs, leave_tqdm):\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    trainbar = tqdm(\n",
    "        iterable=enumerate(context['trainloader'], 0),\n",
    "        total=len(context['trainloader']),\n",
    "        unit=' batches',\n",
    "        desc=f' Epoch {epoch+1}/{n_epochs}',\n",
    "        ascii=True,\n",
    "        position=0,\n",
    "        leave=leave_tqdm\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    # Loop through train batches\n",
    "    for i, (images, labels) in trainbar:\n",
    "        # images = [image.to(device) for image in images]\n",
    "        images = img_handler(images)\n",
    "        labels = label_handler(labels)\n",
    "\n",
    "        output = model(images[0])\n",
    "        loss_dict = criterion(output, labels)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # Zero parameter gradients since PyTorch will just accumulate the gradient\n",
    "        # vectors while it trains (in order to get the \"mean\" direction to move in\n",
    "        # the parameter space). Also doing it this way minimizes memory allocation\n",
    "        # etc, probably.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward() # Computes gradients\n",
    "        optimizer.step() # Do a gradient step\n",
    "\n",
    "        # print statistics\n",
    "        running_train_loss += losses.item()\n",
    "        train_loss = running_train_loss / (i+1)\n",
    "        traintqdminfo = {'train loss':train_loss}\n",
    "        trainbar.set_postfix(traintqdminfo)\n",
    "    \n",
    "    return trainbar, traintqdminfo\n",
    "\n",
    "\n",
    "@interruptable\n",
    "def train_model(\n",
    "        trainloader: DataLoader, \n",
    "        valloader: DataLoader, \n",
    "        model: nn.Module, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        n_epochs: int, \n",
    "        device: torch.device, \n",
    "        validate: bool = True,\n",
    "        save_best: bool = True\n",
    "    ):\n",
    "    \n",
    "    # for convenience\n",
    "    context = {\n",
    "        'trainloader':trainloader,\n",
    "        'valloader':valloader,\n",
    "        'model':model,\n",
    "        'criterion':criterion,\n",
    "        'optimizer':optimizer,\n",
    "        'device':device\n",
    "    }\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainbar, traintqdminfo = _train_model(context, epoch, n_epochs, not validate)\n",
    "            \n",
    "        if validate:\n",
    "            valtqdminfo = _validate_model(context, traintqdminfo)\n",
    "        \n",
    "            # Extra dirty tqdm hack hehe\n",
    "            # _validate_model will create its own tqdm bar that will replace the bar\n",
    "            # from _train_model, but will clear itself afterwards\n",
    "            # the code below reactivates the previous train bar\n",
    "            trainbar.disable = False\n",
    "            trainbar.set_postfix({**traintqdminfo, **valtqdminfo})\n",
    "            trainbar.disable = True\n",
    "            print(file=sys.stderr)\n",
    "        \n",
    "            # Save best models\n",
    "            if save_best:\n",
    "                if valtqdminfo['val loss'] < best_val_loss:\n",
    "                    best_val_loss = valtqdminfo['val loss']\n",
    "                    isodatenow = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                    daydir = datetime.today().strftime(\"weights_%Y-%m-%d\")\n",
    "                    filename = (\n",
    "                        f'detr_statedicts_epoch{epoch+1}'\n",
    "                        f'_train{traintqdminfo[\"train loss\"]:.4f}_val{best_val_loss:.4f}'\n",
    "                        f'_{isodatenow}.pth'\n",
    "                    )\n",
    "                    filepath = os.path.join(WEIGHTS_DIR, daydir, filename)\n",
    "\n",
    "                    save_model(\n",
    "                        obj={\n",
    "                            'model':model.state_dict(),\n",
    "                            'optimizer':optimizer.state_dict(),\n",
    "                            'criterion':criterion.state_dict(),\n",
    "                        },\n",
    "                        f = filepath\n",
    "                    )\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "from torchvision.models import resnet50\n",
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        \n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.stack(inputs, dim=0).to(device)\n",
    "\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        \n",
    "        print(h.shape)\n",
    "    \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}\n",
    "\n",
    "def prepare_model(model, num_classes):\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\n",
    "        url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "        map_location='cpu',\n",
    "        check_hash=True\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load('fish_statedicts/weights_2021-01-06/detr_statedicts_epoch164_train0.2128_val0.3822_2021-01-06T00:58:28.pth')\n",
    "    model.linear_class = nn.Linear(model.linear_bbox.in_features, num_classes + 1)\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    return model\n",
    "    \n",
    "model = DETRdemo(91)\n",
    "model = prepare_model(model, 6)\n",
    "model = model.to(device)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder successfully loaded with pretrained weights\n",
      "Encoder layers are frozen\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "def dict_union_update(a: dict, b: dict):\n",
    "    '''Updates \"a\" with the union of \"a\" and \"b\"'''\n",
    "    a.update((                                   # Set union\n",
    "        (key, b.get(key, a.get(key))) for key in a.keys() & b.keys()\n",
    "    ))\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, pretrained: bool=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers, dropout=0)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        \n",
    "        if pretrained:\n",
    "            self.load_pretrained_weights()\n",
    "    \n",
    "    def load_pretrained_weights(self):\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "            map_location='cpu',\n",
    "            check_hash=True\n",
    "        )\n",
    "    \n",
    "        self_state_dict = self.state_dict()\n",
    "        dict_union_update(self_state_dict, state_dict)\n",
    "        self.load_state_dict(self_state_dict)\n",
    "        print('Encoder successfully loaded with pretrained weights')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.stack(inputs, dim=0).to(device)\n",
    "\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim: int=256):\n",
    "        '''\n",
    "        num_classes: int, should be number of classes WITHOUT \"no object\" class\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        merge_hidden_dim = 64\n",
    "                \n",
    "        # For latent space merging, use 1x1 convs\n",
    "        self.merger1 = nn.Conv2d(in_channels=2, out_channels=merge_hidden_dim, kernel_size=1, stride=1)\n",
    "        self.merger2 = nn.Conv2d(in_channels=merge_hidden_dim, out_channels=merge_hidden_dim, kernel_size=1, stride=1)\n",
    "        self.merger3 = nn.Conv2d(in_channels=merge_hidden_dim, out_channels=1, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.linear_pre_class = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.linear_pre_bbox = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        \n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "    \n",
    "    \n",
    "    def merge(self, h_left, h_right):\n",
    "        # h_left and h_right: (N, 1, 100, 256)\n",
    "        h1 = torch.cat((h_left, h_right), dim=1) # 2 channel out\n",
    "        \n",
    "        h1 = self.merger1(h1) # 64 channel out\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.merger2(h1) # 64 channel out\n",
    "        h2 = F.relu(h1+h2)  # Skip connection\n",
    "        h2 = self.merger3(h2) # 1 channel out\n",
    "        h2 = F.relu(h2)\n",
    "        \n",
    "        # (1, 100, 256)\n",
    "        return h2\n",
    "        \n",
    "    def forward(self, h_left, h_right):\n",
    "        # Output is (N, 1, n_query, n_classes)\n",
    "        h = self.merge(h_left, h_right).squeeze(1)\n",
    "        \n",
    "        h_logits = F.relu(self.linear_pre_class(h))\n",
    "        h_boxes = F.relu(self.linear_pre_bbox(h))\n",
    "    \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h_logits),\n",
    "                'pred_boxes': self.linear_bbox(h_boxes).sigmoid()}\n",
    "    \n",
    "\n",
    "class FishDETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int=256, freeze_encoder: bool=True):\n",
    "        '''\n",
    "        num_classes: int, should be number of classes WITHOUT \"no object\" class\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(6)\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            self.freeze_module(self.encoder)\n",
    "            print('Encoder layers are frozen')\n",
    "            \n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        \n",
    "    @staticmethod\n",
    "    def freeze_module(module):\n",
    "        for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, imgs: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        # (1, 100, 256)\n",
    "        # (batchsize, n_queries, embedding_dim)\n",
    "        # imgs[0] and imgs[1] should be (N, C, H, W)\n",
    "        if self.freeze_encoder:\n",
    "            self.encoder.eval()\n",
    "        \n",
    "        h_left = self.encoder(imgs[0])\n",
    "        h_right = self.encoder(imgs[1])\n",
    "        print(h_left)\n",
    "        return self.decoder(h_left, h_right)\n",
    "    \n",
    "model = FishDETR()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27559685707092285\n",
      "0.2665227949619293\n",
      "0.25914448499679565\n",
      "0.2490002065896988\n",
      "0.23982036113739014\n",
      "0.23112836480140686\n",
      "0.22542798519134521\n",
      "0.21476125717163086\n",
      "0.2086169719696045\n",
      "0.19996708631515503\n",
      "0.19034385681152344\n",
      "0.18267348408699036\n",
      "0.17697805166244507\n",
      "0.17435549199581146\n",
      "0.16871924698352814\n",
      "0.16436243057250977\n",
      "0.15821205079555511\n",
      "0.15389679372310638\n",
      "0.15100528299808502\n",
      "0.14599590003490448\n",
      "0.14146849513053894\n",
      "0.13696816563606262\n",
      "0.13293561339378357\n",
      "0.1293129026889801\n",
      "0.12717631459236145\n",
      "0.12370336055755615\n",
      "0.11751905083656311\n",
      "0.11477477848529816\n",
      "0.11047890037298203\n",
      "0.10551077127456665\n",
      "0.10300648212432861\n",
      "0.09806574881076813\n",
      "0.09424181282520294\n",
      "0.09179787337779999\n",
      "0.08604216575622559\n",
      "0.08361805975437164\n",
      "0.07852591574192047\n",
      "0.07475806027650833\n",
      "0.07037565112113953\n",
      "0.06814232468605042\n",
      "0.06392419338226318\n",
      "0.061916060745716095\n",
      "0.057570017874240875\n",
      "0.055768176913261414\n",
      "0.054081983864307404\n",
      "0.050888460129499435\n",
      "0.04774654656648636\n",
      "0.04791908711194992\n",
      "0.04514262080192566\n",
      "0.04133389890193939\n",
      "0.037574343383312225\n",
      "0.03533438593149185\n",
      "0.03197833150625229\n",
      "0.0322021059691906\n",
      "0.03125008940696716\n",
      "0.02736656181514263\n",
      "0.025711223483085632\n",
      "0.027118951082229614\n",
      "0.025872357189655304\n",
      "0.02270760014653206\n",
      "0.02103513851761818\n",
      "0.019398696720600128\n",
      "0.01817275956273079\n",
      "0.017448849976062775\n",
      "0.01548774540424347\n",
      "0.014893269166350365\n",
      "0.01390889286994934\n",
      "0.011706622317433357\n",
      "0.010939225554466248\n",
      "0.00960302259773016\n",
      "0.008313976228237152\n",
      "0.0059649208560585976\n",
      "0.005189734511077404\n",
      "0.005498903803527355\n",
      "0.005393137224018574\n",
      "0.005801250226795673\n",
      "0.005413278471678495\n",
      "0.005273616407066584\n",
      "0.005113717168569565\n",
      "0.005793140735477209\n",
      "0.005400403402745724\n",
      "0.0048949746415019035\n",
      "0.004702792502939701\n",
      "0.0040474385023117065\n",
      "0.004864729940891266\n",
      "0.00474873511120677\n",
      "0.006054281257092953\n",
      "0.005190887022763491\n",
      "0.007816843688488007\n",
      "0.005484146997332573\n",
      "0.004766817204654217\n",
      "0.005040312185883522\n",
      "0.0029269305523484945\n",
      "0.005501017905771732\n",
      "0.0042408062145113945\n",
      "0.004328756593167782\n",
      "0.0059908488765358925\n",
      "0.0031550826970487833\n",
      "0.0035593481734395027\n"
     ]
    }
   ],
   "source": [
    "@interruptable\n",
    "def train_head(decoder):    \n",
    "    h_lefts = []\n",
    "    h_rights = []\n",
    "    ys = []\n",
    "    \n",
    "    for x, y in traingen:\n",
    "        x = img_handler((x,))[0]\n",
    "        h_lefts.append(model.encoder(x[0]))\n",
    "        h_rights.append(model.encoder(x[1]))\n",
    "        ys.append(y)\n",
    "        \n",
    "\n",
    "    class Gen:\n",
    "        def __len__(self):\n",
    "            return len(ys)\n",
    "\n",
    "        def __getitem__(self, i):\n",
    "            l, r, y = h_lefts[i], h_rights[i], ys[i]\n",
    "            return (l, r), y\n",
    "        \n",
    "    left_tensor = torch.cat(h_lefts).to(device).unsqueeze(1)\n",
    "    right_tensor = torch.cat(h_rights).to(device).unsqueeze(1)\n",
    "    label_list = [{k: v.to(device) for k, v in t.items()} for t in ys]\n",
    "    \n",
    "#     return \n",
    "\n",
    "    decoder.train()\n",
    "    gener = Gen()\n",
    "    \n",
    "    weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "    losses = ['labels', 'boxes', 'cardinality']\n",
    "    matcher = HungarianMatcher()\n",
    "    criterion = SetCriterion(6, matcher, weight_dict, 0.5, losses).to(device)\n",
    "    optimizer = torch.optim.AdamW(decoder.parameters(), lr=1e-5)\n",
    "    \n",
    "    running_train_loss = 0.0\n",
    "    for epoch in range(1,5000):\n",
    "        output = model.decoder(left_tensor, right_tensor)\n",
    "\n",
    "        loss_dict = criterion(output, label_list)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward() # Computes gradients\n",
    "        optimizer.step() # Do a gradient step\n",
    "        \n",
    "        running_train_loss += losses.item()\n",
    "        train_loss = running_train_loss / (epoch)\n",
    "        if not epoch % 50: print(losses.item())\n",
    "            \n",
    "            \n",
    "train_head(model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "matcher = HungarianMatcher()\n",
    "criterion = SetCriterion(6, matcher, weight_dict, eos_coef = 0.5, losses=losses)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=8e-5, momentum=0.001, dampening=0.000001)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "traingen = TorchStereoDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(8,12))\n",
    "valgen = TorchStereoDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(1024,1024+8))\n",
    "\n",
    "# trainloader = DataLoader(\n",
    "#     dataset=traingen,\n",
    "#     batch_size=1,\n",
    "#     collate_fn=lambda x: tuple(zip(*x)),\n",
    "# )\n",
    "\n",
    "# valloader = DataLoader(\n",
    "#     dataset=valgen,\n",
    "#     batch_size=1,\n",
    "#     collate_fn=lambda x: tuple(zip(*x)),\n",
    "# )\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    dataset=traingen,\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "valloader = DataLoader(\n",
    "    dataset=valgen,\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "def sanity_dataset(gen: DataLoader, index=0, classmap: Optional[Mapping[int, str]]=None):\n",
    "    geniter = iter(gen)\n",
    "    \n",
    "    for i in range(index+1):\n",
    "        img, targets = next(geniter)\n",
    "    \n",
    "    img = img.numpy().transpose((1,2,0))\n",
    "    boxes = targets['boxes'].numpy()\n",
    "    labels = targets['labels'].numpy()\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for cls, box in zip(labels,boxes):\n",
    "        cxy = box[:2]\n",
    "        # Visualize center xy\n",
    "        ax.add_patch(plt.Circle(cxy*(h, w), 5, facecolor='red', edgecolor='k', alpha=0.4))\n",
    "        \n",
    "        bw, bh = box[2], box[3]\n",
    "        # tlxy = top left xy\n",
    "        tlxy = (cxy - (bw/2, bh/2))*(h, w)\n",
    "        ax.add_patch(plt.Rectangle(tlxy, bw*w, bh*h, fill=False, lw=2, color='red', alpha=0.4))\n",
    "        \n",
    "        try:\n",
    "            ax.text(*tlxy, classmap[int(cls)], fontsize=11, bbox=dict(facecolor='red', alpha=0.5))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n",
    "sanity_dataset(traingen, 16, num2name)\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=20000,\n",
    "    device=device,\n",
    "    save_best=False,\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model.state_dict(), 'overfit_range_8_12_2in1out.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x: torch.Tensor):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def plot_results(img: Image.Image, classes: Iterable, boxes: Iterable, classmap: Optional[Mapping[int, str]]=None, ax: Optional=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16,10))\n",
    "    img = np.array(img)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    if len(boxes) != 0:\n",
    "        h, w = img.shape[:2]\n",
    "        boxes = box_cxcywh_to_xyxy(boxes)\n",
    "        boxes[:,[0,2]] *= w\n",
    "        boxes[:,[1,3]] *= h\n",
    "        \n",
    "        print([int(cls) for cls in classes])\n",
    "        \n",
    "        for cls, (xmin, ymin, xmax, ymax) in zip(classes, boxes):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                       fill=False, color='cyan', linewidth=3))\n",
    "            try:\n",
    "                ax.text(xmin, ymin, classmap[int(cls)], fontsize=11, bbox=dict(facecolor='cyan', alpha=0.9))\n",
    "            except:\n",
    "                ax.text(xmin, ymin, str(int(cls)), fontsize=11, bbox=dict(facecolor='cyan', alpha=0.9))\n",
    "                pass\n",
    "    \n",
    "    if ax is None:\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "    \n",
    "def postprocess(logits: torch.Tensor, boxes: torch.Tensor):\n",
    "#     return logits.argmax(-1), boxes\n",
    "    keepmask = logits.softmax(-1)[:,:-1].max(-1)[0] > 0.2\n",
    "    if any(keepmask) == False:\n",
    "        return torch.Tensor(), torch.Tensor()\n",
    "    return logits[keepmask].argmax(-1), boxes[keepmask]\n",
    "\n",
    "    \n",
    "def eval_model(model, img: torch.Tensor, classmap: Optional[Mapping[int, str]]=None, ax: Optional=None):    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "#         output = model(img.to(device).unsqueeze(0))\n",
    "        output = model((img[0].to(device), img[1].to(device)))\n",
    "        \n",
    "        boxes = output['pred_boxes'][0]\n",
    "        logits = output['pred_logits'][0]\n",
    "        print(logits.argmax(-1))\n",
    "        \n",
    "        logits_, boxes_ = postprocess(logits, boxes)\n",
    "        \n",
    "        plot_results(img[0][0].cpu().numpy().transpose((1,2,0)), logits_, boxes_, classmap, ax=ax)\n",
    "        \n",
    "# x, y = traingen[16]\n",
    "# eval_model(model, x, num2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3907, -0.5568, -0.0048,  ...,  0.8487, -0.4243,  0.3943],\n",
      "         [ 0.2670,  0.2419,  0.1596,  ...,  0.3186,  0.6097, -0.4375],\n",
      "         [ 0.4054, -0.4681,  0.3535,  ...,  0.7685,  0.2647,  0.4563],\n",
      "         ...,\n",
      "         [ 0.1181,  0.6314,  0.6414,  ...,  0.5057, -0.7100, -0.5046],\n",
      "         [ 0.1330,  0.5709,  0.2368,  ..., -0.0731,  0.7472,  0.2746],\n",
      "         [ 0.1685,  0.0499, -0.1178,  ...,  0.2010, -1.7346, -0.5690]]],\n",
      "       device='cuda:1')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 2, 1, 1], but got 3-dimensional input of size [1, 200, 256] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2cacc6cc8006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0meval_compare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraingen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum2name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-2cacc6cc8006>\u001b[0m in \u001b[0;36meval_compare_model\u001b[0;34m(model, gen, index, classmap)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ca8c5ad9dc20>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, img, classmap, ax)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         output = model(img.to(device).unsqueeze(0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c7d7e902b7d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mh_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFishDETR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c7d7e902b7d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_left, h_right)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Output is (N, 1, n_query, n_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mh_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_pre_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-c7d7e902b7d1>\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, h_left, h_right)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 channel out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerger1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 64 channel out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerger2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 64 channel out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 2, 1, 1], but got 3-dimensional input of size [1, 200, 256] instead"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_model2(model: nn.Module, img: Image.Image): \n",
    "    transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    img_torch = transform(img).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        output = model(img_torch.unsqueeze(0))\n",
    "        \n",
    "    boxes = output['pred_boxes'][0]\n",
    "    logits = output['pred_logits'][0]\n",
    "    logits_, boxes_ = postprocess(logits, boxes)\n",
    "    \n",
    "    \n",
    "    plot_results(np.array(img), logits_, boxes_)\n",
    "    \n",
    "def eval_compare_model(model: nn.Module, gen: Iterable, index: int=0, classmap: Optional[Mapping[int, str]]=None):\n",
    "    x, y = gen[index]\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2,figsize=(15,7))\n",
    "    eval_model(model, x, classmap, axes[0])\n",
    "    plot_results(x[0][0].cpu().numpy().transpose((1,2,0)), y['labels'], y['boxes'], classmap, axes[1])\n",
    "    \n",
    "eval_compare_model(model, traingen, index=2, classmap=num2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img = io.imread('test_image2.png').copy() / 255\n",
    "test_img = Image.open('test_image1.jpg').convert('RGB')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_img_torch = transform(test_img)\n",
    "test_img_torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
