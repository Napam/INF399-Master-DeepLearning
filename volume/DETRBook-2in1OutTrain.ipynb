{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skimage import io\n",
    "\n",
    "################# DETR FUCNTIONS FOR LOSS #######################\n",
    "import sys\n",
    "sys.path.append('./detr_custom/')\n",
    "\n",
    "from models.matcher import HungarianMatcher\n",
    "from models.detr import SetCriterion\n",
    "import engine\n",
    "#################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Glob\n",
    "from glob import glob\n",
    "\n",
    "from typing import Iterable, Sequence, List, Tuple, Dict, Optional, Union, Any, Callable, Mapping\n",
    "from types import ModuleType\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from generators import BlenderStandardDataset, TorchStandardDataset, TorchStereoDataset\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "import sqlite3 as db\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def pytorch_init_janus_gpu():\n",
    "    device_id = 1\n",
    "    torch.cuda.set_device(device_id)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert torch.cuda.current_device() == 1, 'Using wrong GPU'\n",
    "    assert torch.cuda.device_count() == 2, 'Cannot find both GPUs'\n",
    "    assert torch.cuda.get_device_name(0) == 'GeForce RTX 2080 Ti', 'Unexpected GPU name'\n",
    "    assert torch.cuda.is_available() == True, 'GPU not available'\n",
    "    return torch.device('cuda', device_id)\n",
    "\n",
    "\n",
    "def reloader(module_or_member: Union[ModuleType, Any]):    \n",
    "    if isinstance(module_or_member, ModuleType):\n",
    "        importlib.reload(module_or_member)\n",
    "        return module\n",
    "    else:\n",
    "        module = importlib.import_module(module_or_member.__module__)\n",
    "        importlib.reload(module)\n",
    "        return module.__dict__[module_or_member.__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name('cuda:1'))\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1 (GeForce RTX 2080 Ti)\n"
     ]
    }
   ],
   "source": [
    "seed = 42069\n",
    "seed_everything(seed)\n",
    "\n",
    "try:\n",
    "    device = pytorch_init_janus_gpu()\n",
    "    print(f'Using device: {device} ({torch.cuda.get_device_name()})')\n",
    "except AssertionError as e:\n",
    "    print('GPU could not initialize, got error:', e)\n",
    "    device = torch.device('cpu')\n",
    "    print('Device is set to CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_CACHE_DIR = 'torch_cache'\n",
    "DATASET_DIR = '/mnt/blendervol/objdet_std_data'\n",
    "DATASET_DIR = '/mnt/blendervol/leftright_left_data'\n",
    "SQL_TABLE = 'bboxes_std'\n",
    "WEIGHTS_DIR = 'fish_statedicts'\n",
    "NUM_CLASSES=6+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = db.connect(f'file:{os.path.join(DATASET_DIR,\"bboxes.db\")}?mode=ro', uri=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2name = eval(open(os.path.join(DATASET_DIR,\"metadata.txt\"), 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = pd.read_sql_query('SELECT COUNT(DISTINCT(imgnr)) FROM bboxes_std', db_con).values[0][0]\n",
    "\n",
    "TRAIN_RANGE = (0, int(3/4*n_data))\n",
    "VAL_RANGE = (int(3/4*n_data), n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir(TORCH_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  \n",
    "class DETRModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        self.model = torch.hub.load(\n",
    "            os.path.join(TORCH_CACHE_DIR, 'facebookresearch_detr_master'), \n",
    "            model='detr_resnet50', \n",
    "            pretrained=True,\n",
    "            source='local'\n",
    "        )\n",
    "        \n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        \n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)\n",
    "\n",
    "model = DETRModel(NUM_CLASSES, 100)\n",
    "model = model.to(device)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(obj: Any, f: str):\n",
    "    pathlib.Path(f).parent.mkdir(parents=True, exist_ok=True)\n",
    "    assert isinstance(f, str), \"Filename must be of type string when saving model\"\n",
    "    torch.save(obj=obj, f=f)\n",
    "    \n",
    "    \n",
    "def interruptable(f: Callable):\n",
    "    '''Decorator for functions that should handle KeyboardInterrupts more gracefully'''\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            retval = f(*args, **kwargs)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print('Process interrupted')\n",
    "            return \n",
    "        return retval\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def img_handler(images: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    # pp for pre process\n",
    "    pp = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.Normalize([0.485, 0.456, 0.406], \n",
    "                    [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return [\n",
    "        (pp(pair[0]).to(device), pp(pair[1]).to(device)) \n",
    "        for pair in images\n",
    "    ]\n",
    "    \n",
    "    \n",
    "def label_handler(labels):\n",
    "    return [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "    \n",
    "def _validate_model(context, traintqdminfo):\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # valbar will disappear after it is done since leave=False\n",
    "    valbar = tqdm(\n",
    "        iterable=enumerate(context['valloader'], 0), \n",
    "        total=len(context['valloader']), \n",
    "        unit=' batches',\n",
    "        desc=f' Validating',\n",
    "        ascii=True,\n",
    "        position=0,\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    model = context['model']\n",
    "    criterion = context['criterion']\n",
    "    \n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    # Loop through val batches\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in valbar:\n",
    "            # images = [image.to(device) for image in images]\n",
    "            images = img_handler(images)\n",
    "            labels = label_handler(labels)\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            loss_dict = criterion(output, labels)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "            # print statistics\n",
    "            running_val_loss += losses.item()\n",
    "            val_loss = running_val_loss / (i+1)\n",
    "            valtqdminfo = {**traintqdminfo, 'val loss':val_loss}\n",
    "            valbar.set_postfix(valtqdminfo)\n",
    "            \n",
    "    return valtqdminfo\n",
    "\n",
    "\n",
    "def _train_model(context, epoch, n_epochs, leave_tqdm):\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    trainbar = tqdm(\n",
    "        iterable=enumerate(context['trainloader'], 0),\n",
    "        total=len(context['trainloader']),\n",
    "        unit=' batches',\n",
    "        desc=f' Epoch {epoch+1}/{n_epochs}',\n",
    "        ascii=True,\n",
    "        position=0,\n",
    "        leave=leave_tqdm\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    # Loop through train batches\n",
    "    for i, (images, labels) in trainbar:\n",
    "        # images = [image.to(device) for image in images]\n",
    "        images = img_handler(images)\n",
    "        labels = label_handler(labels)\n",
    "\n",
    "        output = model(images[0])\n",
    "        loss_dict = criterion(output, labels)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # Zero parameter gradients since PyTorch will just accumulate the gradient\n",
    "        # vectors while it trains (in order to get the \"mean\" direction to move in\n",
    "        # the parameter space). Also doing it this way minimizes memory allocation\n",
    "        # etc, probably.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward() # Computes gradients\n",
    "        optimizer.step() # Do a gradient step\n",
    "\n",
    "        # print statistics\n",
    "        running_train_loss += losses.item()\n",
    "        train_loss = running_train_loss / (i+1)\n",
    "        traintqdminfo = {'train loss':train_loss}\n",
    "        trainbar.set_postfix(traintqdminfo)\n",
    "    \n",
    "    return trainbar, traintqdminfo\n",
    "\n",
    "\n",
    "@interruptable\n",
    "def train_model(\n",
    "        trainloader: DataLoader, \n",
    "        valloader: DataLoader, \n",
    "        model: nn.Module, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        n_epochs: int, \n",
    "        device: torch.device, \n",
    "        validate: bool = True,\n",
    "        save_best: bool = True\n",
    "    ):\n",
    "    \n",
    "    # for convenience\n",
    "    context = {\n",
    "        'trainloader':trainloader,\n",
    "        'valloader':valloader,\n",
    "        'model':model,\n",
    "        'criterion':criterion,\n",
    "        'optimizer':optimizer,\n",
    "        'device':device\n",
    "    }\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainbar, traintqdminfo = _train_model(context, epoch, n_epochs, not validate)\n",
    "            \n",
    "        if validate:\n",
    "            valtqdminfo = _validate_model(context, traintqdminfo)\n",
    "        \n",
    "            # Extra dirty tqdm hack hehe\n",
    "            # _validate_model will create its own tqdm bar that will replace the bar\n",
    "            # from _train_model, but will clear itself afterwards\n",
    "            # the code below reactivates the previous train bar\n",
    "            trainbar.disable = False\n",
    "            trainbar.set_postfix({**traintqdminfo, **valtqdminfo})\n",
    "            trainbar.disable = True\n",
    "            print(file=sys.stderr)\n",
    "        \n",
    "            # Save best models\n",
    "            if save_best:\n",
    "                if valtqdminfo['val loss'] < best_val_loss:\n",
    "                    best_val_loss = valtqdminfo['val loss']\n",
    "                    isodatenow = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                    daydir = datetime.today().strftime(\"weights_%Y-%m-%d\")\n",
    "                    filename = (\n",
    "                        f'detr_statedicts_epoch{epoch+1}'\n",
    "                        f'_train{traintqdminfo[\"train loss\"]:.4f}_val{best_val_loss:.4f}'\n",
    "                        f'_{isodatenow}.pth'\n",
    "                    )\n",
    "                    filepath = os.path.join(WEIGHTS_DIR, daydir, filename)\n",
    "\n",
    "                    save_model(\n",
    "                        obj={\n",
    "                            'model':model.state_dict(),\n",
    "                            'optimizer':optimizer.state_dict(),\n",
    "                            'criterion':criterion.state_dict(),\n",
    "                        },\n",
    "                        f = filepath\n",
    "                    )\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "from torchvision.models import resnet50\n",
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        \n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.stack(inputs, dim=0).to(device)\n",
    "\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        \n",
    "        print(h.shape)\n",
    "    \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}\n",
    "\n",
    "def prepare_model(model, num_classes):\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\n",
    "        url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "        map_location='cpu',\n",
    "        check_hash=True\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load('fish_statedicts/weights_2021-01-06/detr_statedicts_epoch164_train0.2128_val0.3822_2021-01-06T00:58:28.pth')\n",
    "    model.linear_class = nn.Linear(model.linear_bbox.in_features, num_classes + 1)\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    return model\n",
    "    \n",
    "model = DETRdemo(91)\n",
    "model = prepare_model(model, 6)\n",
    "model = model.to(device)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder successfully loaded with pretrained weights\n",
      "Encoder layers are frozen\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "def dict_union_update(a: dict, b: dict):\n",
    "    '''Updates \"a\" with the union of \"a\" and \"b\"'''\n",
    "    a.update((                                   # Set union\n",
    "        (key, b.get(key, a.get(key))) for key in a.keys() & b.keys()\n",
    "    ))\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, pretrained: bool=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers, dropout=0)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        \n",
    "        if pretrained:\n",
    "            self.load_pretrained_weights()\n",
    "    \n",
    "    def load_pretrained_weights(self):\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "            map_location='cpu',\n",
    "            check_hash=True\n",
    "        )\n",
    "    \n",
    "        self_state_dict = self.state_dict()\n",
    "        dict_union_update(self_state_dict, state_dict)\n",
    "        self.load_state_dict(self_state_dict)\n",
    "        print('Encoder successfully loaded with pretrained weights')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.stack(inputs, dim=0).to(device)\n",
    "\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim: int=256):\n",
    "        '''\n",
    "        num_classes: int, should be number of classes WITHOUT \"no object\" class\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        merge_hidden_dim = 64\n",
    "                \n",
    "        # For latent space merging, use 1x1 convs\n",
    "        self.merger1 = nn.Conv2d(in_channels=2, out_channels=merge_hidden_dim, kernel_size=1, stride=1)\n",
    "        self.merger2 = nn.Conv2d(in_channels=merge_hidden_dim, out_channels=merge_hidden_dim, kernel_size=1, stride=1)\n",
    "        self.merger3 = nn.Conv2d(in_channels=merge_hidden_dim, out_channels=1, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.linear_pre_class = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.linear_pre_bbox = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        \n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "    \n",
    "    \n",
    "    def merge(self, h_left, h_right):\n",
    "        # (2, 100, 256)\n",
    "        h1 = torch.cat((h_left, h_right), dim=0).unsqueeze(0) # 2 channel out\n",
    "        \n",
    "        h1 = self.merger1(h1) # 64 channel out\n",
    "        h1 = F.relu(h1)\n",
    "        h2 = self.merger2(h1) # 64 channel out\n",
    "        h2 = F.relu(h1+h2)  # Skip connection\n",
    "        h2 = self.merger3(h2) # 1 channel out\n",
    "        h2 = F.relu(h2)\n",
    "        \n",
    "        # (1, 100, 256)\n",
    "        return h2\n",
    "        \n",
    "    def forward(self, h_left, h_right):\n",
    "        h = self.merge(h_left, h_right)[0]\n",
    "        \n",
    "        h_logits = F.relu(self.linear_pre_class(h))\n",
    "        h_boxes = F.relu(self.linear_pre_bbox(h))\n",
    "        \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h_logits),\n",
    "                'pred_boxes': self.linear_bbox(h_boxes).sigmoid()}\n",
    "    \n",
    "\n",
    "class FishDETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int=256, freeze_encoder: bool=True):\n",
    "        '''\n",
    "        num_classes: int, should be number of classes WITHOUT \"no object\" class\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(6)\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            self.freeze_module(self.encoder)\n",
    "            print('Encoder layers are frozen')\n",
    "            \n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        \n",
    "    @staticmethod\n",
    "    def freeze_module(module):\n",
    "        for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, imgs: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        # (1, 100, 256)\n",
    "        # (batchsize, n_queries, embedding_dim)\n",
    "        # imgs[0] and imgs[1] should be (N, C, H, W)\n",
    "        if self.freeze_encoder:\n",
    "            self.encoder.eval()\n",
    "        \n",
    "        h_left = self.encoder(imgs[0])\n",
    "        h_right = self.encoder(imgs[1])\n",
    "        return self.decoder(h_left, h_right)\n",
    "    \n",
    "model = FishDETR()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_logits': tensor([[[-0.0055,  0.0317, -0.1104,  0.0643,  0.0784,  0.0772,  0.0270],\n",
      "         [-0.0745, -0.0170, -0.0495,  0.0999,  0.0339,  0.0738,  0.0606],\n",
      "         [-0.0047,  0.0154, -0.0515,  0.0783,  0.0229,  0.0454,  0.0361],\n",
      "         [-0.0404,  0.0254, -0.1163,  0.0439,  0.0073,  0.1088, -0.0013],\n",
      "         [-0.0848,  0.0111, -0.0510,  0.1010,  0.0371,  0.0487,  0.0702],\n",
      "         [-0.0454,  0.0451, -0.0562,  0.0463,  0.0297,  0.0905,  0.0269],\n",
      "         [-0.0906,  0.0159, -0.0253,  0.0988,  0.0252,  0.0689,  0.0359],\n",
      "         [-0.0767, -0.0294, -0.0588,  0.1293,  0.0773,  0.0521,  0.0512],\n",
      "         [-0.0451,  0.0055, -0.0693,  0.0605, -0.0047,  0.0734,  0.0346],\n",
      "         [-0.0168, -0.0376, -0.0454,  0.1020,  0.0459,  0.0477,  0.0587],\n",
      "         [-0.0547, -0.0146, -0.0568,  0.1407,  0.0705,  0.0509,  0.0544],\n",
      "         [-0.1000, -0.0161, -0.0646,  0.1128,  0.0853,  0.0430,  0.0562],\n",
      "         [-0.0443,  0.0444, -0.1228,  0.0410,  0.0004,  0.0966, -0.0014],\n",
      "         [-0.0031, -0.0008, -0.0536,  0.0766,  0.0126,  0.0400,  0.0464],\n",
      "         [-0.0447, -0.0091, -0.0571,  0.0928,  0.0595,  0.0084,  0.0494],\n",
      "         [-0.0665, -0.0143, -0.0513,  0.0870,  0.0030,  0.0656,  0.0487],\n",
      "         [-0.0730,  0.0053, -0.0426,  0.1079,  0.0919,  0.0482,  0.0569],\n",
      "         [-0.0202, -0.0161, -0.0760,  0.0819, -0.0228,  0.0630,  0.0491],\n",
      "         [-0.0772,  0.0006, -0.0483,  0.1032,  0.0850,  0.0298,  0.0607],\n",
      "         [-0.0739, -0.0061, -0.0650,  0.0722,  0.0361,  0.0591,  0.0320],\n",
      "         [-0.0366,  0.0051, -0.0607,  0.0588,  0.0297,  0.0778,  0.0341],\n",
      "         [-0.0516,  0.0206, -0.0626,  0.0515,  0.0277,  0.0776,  0.0313],\n",
      "         [-0.0414,  0.0111, -0.0544,  0.0619,  0.0289,  0.0808,  0.0283],\n",
      "         [-0.1065, -0.0052, -0.0443,  0.1361,  0.0402,  0.0355,  0.0758],\n",
      "         [-0.0447,  0.0020, -0.0605,  0.0698,  0.0071,  0.0686,  0.0420],\n",
      "         [-0.0256, -0.0395, -0.0480,  0.0958,  0.0415,  0.0607,  0.0591],\n",
      "         [-0.0335,  0.0175, -0.0433,  0.1134,  0.0297,  0.0648,  0.0669],\n",
      "         [-0.0897, -0.0179, -0.0175,  0.1343,  0.0468,  0.0767,  0.0278],\n",
      "         [-0.0619,  0.0068, -0.0585,  0.0816,  0.0317,  0.0724,  0.0522],\n",
      "         [-0.0790, -0.0219, -0.0252,  0.1374,  0.0672,  0.0704,  0.0778],\n",
      "         [-0.0613,  0.0069, -0.0921,  0.1002,  0.0644,  0.0603,  0.0562],\n",
      "         [-0.0171,  0.0113, -0.0692,  0.0808,  0.0050,  0.0477,  0.0427],\n",
      "         [-0.1168,  0.0116, -0.0319,  0.1417,  0.0459,  0.0419,  0.0515],\n",
      "         [-0.1001, -0.0191, -0.0571,  0.1077,  0.0392,  0.0511,  0.0702],\n",
      "         [-0.0081,  0.0306, -0.1050,  0.0632,  0.0773,  0.0739,  0.0255],\n",
      "         [-0.0293,  0.0064, -0.0495,  0.0909,  0.0077,  0.0615,  0.0499],\n",
      "         [-0.0593,  0.0344, -0.0816,  0.0591, -0.0032,  0.0971,  0.0262],\n",
      "         [-0.0206, -0.0043, -0.0455,  0.0781,  0.0501,  0.0509,  0.0539],\n",
      "         [-0.0672,  0.0342, -0.0616,  0.0838,  0.0003,  0.0952,  0.0396],\n",
      "         [-0.0669,  0.0544, -0.0411,  0.0826,  0.0300,  0.0758,  0.0316],\n",
      "         [-0.0054, -0.0232, -0.0577,  0.0908, -0.0014,  0.0565,  0.0589],\n",
      "         [-0.0517, -0.0205, -0.0580,  0.1229,  0.0279,  0.0850,  0.0605],\n",
      "         [-0.0880, -0.0137, -0.0437,  0.1182,  0.0428,  0.0708,  0.0741],\n",
      "         [-0.0422,  0.0382, -0.0538,  0.0433,  0.0256,  0.0946,  0.0279],\n",
      "         [-0.0789, -0.0080, -0.0464,  0.1164,  0.0843,  0.0210,  0.0634],\n",
      "         [-0.0356, -0.0449, -0.0668,  0.1148,  0.0426,  0.0774,  0.0486],\n",
      "         [-0.0111,  0.0195, -0.0626,  0.0699, -0.0347,  0.0695,  0.0469],\n",
      "         [-0.0243, -0.0370, -0.0507,  0.1104,  0.0377,  0.0503,  0.0602],\n",
      "         [-0.0684, -0.0055, -0.0629,  0.0753,  0.0337,  0.0646,  0.0386],\n",
      "         [-0.0521,  0.0106, -0.0589,  0.0599,  0.0079,  0.0744,  0.0321],\n",
      "         [-0.0562,  0.0155, -0.0619,  0.0686,  0.0258,  0.0749,  0.0282],\n",
      "         [-0.0425,  0.0124, -0.0695,  0.0544,  0.0337,  0.0706,  0.0331],\n",
      "         [-0.0159, -0.0281, -0.0469,  0.1038,  0.0270,  0.0499,  0.0522],\n",
      "         [-0.0126,  0.0356, -0.1040,  0.0676,  0.0915,  0.0563,  0.0274],\n",
      "         [-0.0707, -0.0184, -0.0571,  0.1355,  0.0143,  0.0554,  0.0799],\n",
      "         [-0.1203, -0.0009, -0.0410,  0.1342,  0.0393,  0.0364,  0.0621],\n",
      "         [-0.0196,  0.0426, -0.1125,  0.0464, -0.0074,  0.0894,  0.0227],\n",
      "         [-0.0626, -0.0291, -0.0488,  0.1390,  0.0676,  0.0851,  0.0557],\n",
      "         [-0.0327,  0.0159, -0.1080,  0.0892,  0.0655,  0.0405,  0.0410],\n",
      "         [-0.0995,  0.0082, -0.0284,  0.1181,  0.0292,  0.0471,  0.0549],\n",
      "         [-0.0940, -0.0210, -0.0436,  0.1520,  0.0872,  0.0588,  0.0457],\n",
      "         [-0.0752,  0.0010, -0.0372,  0.0819,  0.0355,  0.0767,  0.0465],\n",
      "         [-0.0200, -0.0086, -0.0460,  0.0849,  0.0146,  0.0774,  0.0540],\n",
      "         [-0.0827,  0.0252, -0.0093,  0.1538,  0.0569,  0.0749,  0.0466],\n",
      "         [-0.0483,  0.0311, -0.1045,  0.0552, -0.0056,  0.0926,  0.0117],\n",
      "         [-0.0559,  0.0008, -0.0669,  0.0636,  0.0071,  0.0819,  0.0343],\n",
      "         [-0.0257,  0.0086, -0.0770,  0.0830, -0.0300,  0.0662,  0.0376],\n",
      "         [-0.0554,  0.0186, -0.0760,  0.0591,  0.0016,  0.0876,  0.0321],\n",
      "         [-0.0494, -0.0059, -0.0620,  0.0749, -0.0060,  0.0725,  0.0436],\n",
      "         [-0.0279,  0.0479, -0.1136,  0.0485, -0.0150,  0.0891,  0.0249],\n",
      "         [-0.0605, -0.0127, -0.0394,  0.1304,  0.0594,  0.0802,  0.0592],\n",
      "         [-0.0225,  0.0326, -0.0691,  0.0705, -0.0474,  0.0732,  0.0525],\n",
      "         [-0.0698, -0.0161, -0.0236,  0.1444,  0.0852,  0.0800,  0.0548],\n",
      "         [-0.0111, -0.0138, -0.0600,  0.0753,  0.0244,  0.0358,  0.0475],\n",
      "         [-0.0007,  0.0310, -0.0596,  0.0839,  0.0077,  0.0532,  0.0480],\n",
      "         [-0.0570, -0.0169, -0.0589,  0.1168,  0.0210,  0.0883,  0.0680],\n",
      "         [-0.0327, -0.0016, -0.0546,  0.0808,  0.0029,  0.0635,  0.0485],\n",
      "         [-0.0995,  0.0164, -0.0315,  0.1266,  0.0493,  0.0511,  0.0559],\n",
      "         [-0.0058,  0.0317, -0.1033,  0.0584,  0.0500,  0.0682,  0.0249],\n",
      "         [-0.0711, -0.0132, -0.0718,  0.0864,  0.0404,  0.0493,  0.0474],\n",
      "         [-0.0450,  0.0098, -0.0759,  0.0676, -0.0035,  0.0777,  0.0378],\n",
      "         [-0.0222,  0.0297, -0.0609,  0.0717, -0.0218,  0.0715,  0.0452],\n",
      "         [-0.0395, -0.0207, -0.0378,  0.0913,  0.0650,  0.0449,  0.0546],\n",
      "         [-0.0129,  0.0013, -0.0475,  0.0737,  0.0368,  0.0448,  0.0471],\n",
      "         [-0.0601, -0.0161, -0.0715,  0.0821,  0.0463,  0.0442,  0.0496],\n",
      "         [-0.0385,  0.0096, -0.0463,  0.0692,  0.0859,  0.0263,  0.0424],\n",
      "         [-0.1054, -0.0205, -0.0654,  0.0995,  0.0911,  0.0434,  0.0487],\n",
      "         [-0.0824,  0.0004, -0.0251,  0.1457,  0.0517,  0.0781,  0.0623],\n",
      "         [-0.0556,  0.0052, -0.0240,  0.1367,  0.1017,  0.0522,  0.0437],\n",
      "         [-0.0776, -0.0159, -0.0337,  0.1410,  0.0577,  0.0678,  0.0692],\n",
      "         [-0.0028, -0.0121, -0.0592,  0.0905, -0.0002,  0.0543,  0.0545],\n",
      "         [-0.0668,  0.0588, -0.0314,  0.0758,  0.0296,  0.0771,  0.0296],\n",
      "         [-0.0149,  0.0124, -0.0687,  0.0867, -0.0119,  0.0631,  0.0381],\n",
      "         [-0.0031,  0.0119, -0.0610,  0.0692, -0.0355,  0.0613,  0.0515],\n",
      "         [-0.0146, -0.0031, -0.0605,  0.0721, -0.0223,  0.0622,  0.0476],\n",
      "         [-0.0411,  0.0052, -0.0509,  0.0647,  0.0887,  0.0346,  0.0416],\n",
      "         [-0.0637, -0.0066, -0.0474,  0.1006,  0.0970,  0.0232,  0.0559],\n",
      "         [-0.0897,  0.0019, -0.0648,  0.1021,  0.0379,  0.0540,  0.0733],\n",
      "         [-0.0847, -0.0220, -0.0468,  0.0927,  0.0361,  0.0628,  0.0480],\n",
      "         [-0.0837, -0.0208, -0.0809,  0.1018, -0.0100,  0.0749,  0.0512]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), 'pred_boxes': tensor([[[0.4955, 0.5020, 0.4991, 0.5074],\n",
      "         [0.4975, 0.4926, 0.5028, 0.4933],\n",
      "         [0.4975, 0.4958, 0.5018, 0.5020],\n",
      "         [0.4931, 0.5078, 0.5001, 0.4989],\n",
      "         [0.5015, 0.4957, 0.5049, 0.4953],\n",
      "         [0.4871, 0.5002, 0.5053, 0.5089],\n",
      "         [0.4950, 0.4836, 0.5090, 0.5032],\n",
      "         [0.5067, 0.4910, 0.5011, 0.4878],\n",
      "         [0.4889, 0.4981, 0.5065, 0.4954],\n",
      "         [0.5014, 0.4923, 0.4978, 0.4906],\n",
      "         [0.5061, 0.4879, 0.4947, 0.4889],\n",
      "         [0.5071, 0.4840, 0.5061, 0.4899],\n",
      "         [0.4950, 0.5078, 0.5013, 0.4996],\n",
      "         [0.4969, 0.4941, 0.5023, 0.5009],\n",
      "         [0.5007, 0.4874, 0.4929, 0.4902],\n",
      "         [0.4953, 0.4944, 0.5021, 0.4920],\n",
      "         [0.5028, 0.4846, 0.5039, 0.4919],\n",
      "         [0.4949, 0.4940, 0.5058, 0.4957],\n",
      "         [0.5073, 0.4857, 0.4951, 0.4901],\n",
      "         [0.4918, 0.4915, 0.5028, 0.4957],\n",
      "         [0.4869, 0.4993, 0.5038, 0.5004],\n",
      "         [0.4899, 0.4958, 0.5036, 0.5011],\n",
      "         [0.4898, 0.5042, 0.5026, 0.4987],\n",
      "         [0.4992, 0.4924, 0.5055, 0.4988],\n",
      "         [0.4942, 0.4951, 0.5057, 0.4957],\n",
      "         [0.5009, 0.4913, 0.5008, 0.4910],\n",
      "         [0.5037, 0.4962, 0.5010, 0.4858],\n",
      "         [0.5147, 0.5033, 0.5097, 0.4943],\n",
      "         [0.4898, 0.4933, 0.5086, 0.5036],\n",
      "         [0.5125, 0.5009, 0.5108, 0.4917],\n",
      "         [0.5004, 0.5006, 0.5001, 0.4970],\n",
      "         [0.4972, 0.4956, 0.5049, 0.5023],\n",
      "         [0.5108, 0.4916, 0.5084, 0.4934],\n",
      "         [0.4989, 0.4941, 0.5025, 0.4952],\n",
      "         [0.4951, 0.5014, 0.4982, 0.5077],\n",
      "         [0.5005, 0.4936, 0.5027, 0.4956],\n",
      "         [0.4911, 0.5022, 0.5054, 0.4993],\n",
      "         [0.4962, 0.4932, 0.5002, 0.4992],\n",
      "         [0.4904, 0.4981, 0.5080, 0.5077],\n",
      "         [0.4965, 0.4988, 0.5126, 0.5080],\n",
      "         [0.4959, 0.4929, 0.5068, 0.4938],\n",
      "         [0.5068, 0.4956, 0.5043, 0.4876],\n",
      "         [0.5051, 0.4943, 0.5067, 0.4929],\n",
      "         [0.4858, 0.5006, 0.5066, 0.5089],\n",
      "         [0.5063, 0.4891, 0.4956, 0.4913],\n",
      "         [0.5013, 0.4909, 0.5004, 0.4902],\n",
      "         [0.4957, 0.4953, 0.5075, 0.4935],\n",
      "         [0.5026, 0.4902, 0.5023, 0.4935],\n",
      "         [0.4902, 0.4902, 0.5051, 0.4967],\n",
      "         [0.4907, 0.4990, 0.5062, 0.4978],\n",
      "         [0.4913, 0.4922, 0.5064, 0.4989],\n",
      "         [0.4888, 0.4970, 0.5068, 0.5009],\n",
      "         [0.5004, 0.4894, 0.5016, 0.4939],\n",
      "         [0.4933, 0.4967, 0.4971, 0.5042],\n",
      "         [0.5071, 0.4931, 0.5010, 0.4895],\n",
      "         [0.5006, 0.4873, 0.5062, 0.4982],\n",
      "         [0.4963, 0.5085, 0.5018, 0.5029],\n",
      "         [0.5157, 0.5008, 0.5119, 0.4928],\n",
      "         [0.4959, 0.4934, 0.4936, 0.4947],\n",
      "         [0.5006, 0.4897, 0.5047, 0.4970],\n",
      "         [0.5149, 0.4891, 0.5039, 0.4876],\n",
      "         [0.4918, 0.4840, 0.5082, 0.5014],\n",
      "         [0.4953, 0.4923, 0.5089, 0.4919],\n",
      "         [0.5127, 0.4899, 0.5122, 0.4918],\n",
      "         [0.4930, 0.5060, 0.4996, 0.5024],\n",
      "         [0.4872, 0.4936, 0.5063, 0.4974],\n",
      "         [0.4951, 0.4948, 0.5059, 0.5010],\n",
      "         [0.4920, 0.5024, 0.5063, 0.4964],\n",
      "         [0.4929, 0.4966, 0.5072, 0.4944],\n",
      "         [0.4974, 0.5077, 0.5022, 0.5033],\n",
      "         [0.5147, 0.5015, 0.5082, 0.4888],\n",
      "         [0.4927, 0.4974, 0.5068, 0.5008],\n",
      "         [0.5142, 0.4859, 0.5105, 0.4931],\n",
      "         [0.4964, 0.4931, 0.5011, 0.4997],\n",
      "         [0.4960, 0.4985, 0.5039, 0.5035],\n",
      "         [0.5034, 0.4962, 0.5078, 0.4896],\n",
      "         [0.4984, 0.4948, 0.5079, 0.4932],\n",
      "         [0.5062, 0.4932, 0.5075, 0.4982],\n",
      "         [0.4958, 0.5040, 0.4989, 0.5083],\n",
      "         [0.4922, 0.4916, 0.5054, 0.4951],\n",
      "         [0.4950, 0.4984, 0.5054, 0.4954],\n",
      "         [0.4990, 0.4972, 0.5083, 0.4959],\n",
      "         [0.4959, 0.4886, 0.5010, 0.4898],\n",
      "         [0.4974, 0.4951, 0.5013, 0.5019],\n",
      "         [0.5115, 0.4950, 0.5119, 0.4848],\n",
      "         [0.4959, 0.4877, 0.4927, 0.4943],\n",
      "         [0.5065, 0.4824, 0.5055, 0.4897],\n",
      "         [0.5117, 0.5017, 0.5120, 0.4975],\n",
      "         [0.5144, 0.4977, 0.5023, 0.4863],\n",
      "         [0.5069, 0.4935, 0.5094, 0.4979],\n",
      "         [0.4965, 0.4931, 0.5060, 0.4967],\n",
      "         [0.4937, 0.4980, 0.5134, 0.5057],\n",
      "         [0.5002, 0.4960, 0.5051, 0.5028],\n",
      "         [0.4948, 0.4949, 0.5058, 0.4968],\n",
      "         [0.4957, 0.4945, 0.5045, 0.4971],\n",
      "         [0.4972, 0.4866, 0.4948, 0.4954],\n",
      "         [0.5044, 0.4873, 0.4942, 0.4894],\n",
      "         [0.4997, 0.4940, 0.5055, 0.4989],\n",
      "         [0.4949, 0.4909, 0.5057, 0.4937],\n",
      "         [0.4958, 0.4944, 0.5026, 0.4967]]], device='cuda:1',\n",
      "       grad_fn=<SigmoidBackward>)}\n",
      "{'pred_logits': tensor([[[-0.0056,  0.0437, -0.1066,  0.0413,  0.1040,  0.0502,  0.0194],\n",
      "         [-0.0709, -0.0025, -0.0474,  0.0927, -0.0088,  0.0898,  0.0545],\n",
      "         [-0.0016,  0.0027, -0.0550,  0.0778,  0.0437,  0.0474,  0.0466],\n",
      "         [-0.0241,  0.0192, -0.1079,  0.0456,  0.0032,  0.1055,  0.0323],\n",
      "         [-0.0960, -0.0089, -0.0712,  0.0938,  0.0322,  0.0479,  0.0619],\n",
      "         [-0.0389,  0.0400, -0.0817,  0.0538,  0.0272,  0.1137,  0.0504],\n",
      "         [-0.0703,  0.0061, -0.0529,  0.0822,  0.0213,  0.0970,  0.0569],\n",
      "         [-0.0467, -0.0325, -0.0520,  0.1216,  0.0413,  0.0711,  0.0549],\n",
      "         [-0.0546,  0.0050, -0.0747,  0.0653, -0.0093,  0.0786,  0.0377],\n",
      "         [-0.0417, -0.0253, -0.0618,  0.0973,  0.0353,  0.0593,  0.0468],\n",
      "         [-0.0477, -0.0188, -0.0582,  0.1326,  0.0392,  0.0851,  0.0481],\n",
      "         [-0.0654,  0.0192, -0.0292,  0.1607,  0.0187,  0.0835,  0.0492],\n",
      "         [-0.0177,  0.0403, -0.1051,  0.0585, -0.0021,  0.0901,  0.0288],\n",
      "         [-0.0053,  0.0108, -0.0569,  0.0798,  0.0164,  0.0548,  0.0408],\n",
      "         [-0.0605,  0.0155, -0.0874,  0.0978,  0.0635,  0.0288,  0.0568],\n",
      "         [-0.0213, -0.0044, -0.0599,  0.0830, -0.0051,  0.0680,  0.0529],\n",
      "         [-0.0680, -0.0006, -0.0701,  0.0787,  0.0881,  0.0381,  0.0490],\n",
      "         [-0.0082, -0.0093, -0.0702,  0.0697, -0.0049,  0.0727,  0.0443],\n",
      "         [-0.0742,  0.0052, -0.0552,  0.1101,  0.0911,  0.0359,  0.0553],\n",
      "         [-0.0511, -0.0086, -0.0656,  0.0803,  0.0026,  0.0854,  0.0410],\n",
      "         [-0.0384,  0.0149, -0.0834,  0.0601,  0.0276,  0.1082,  0.0526],\n",
      "         [-0.0325,  0.0283, -0.0800,  0.0591,  0.0258,  0.1096,  0.0463],\n",
      "         [-0.0246,  0.0348, -0.0888,  0.0435,  0.0343,  0.1057,  0.0336],\n",
      "         [-0.1014, -0.0143, -0.0558,  0.1188,  0.0376,  0.0486,  0.0686],\n",
      "         [-0.0421, -0.0020, -0.0600,  0.0773, -0.0071,  0.0703,  0.0439],\n",
      "         [-0.0349, -0.0228, -0.0599,  0.1237,  0.0182,  0.0881,  0.0455],\n",
      "         [-0.0484, -0.0078, -0.0622,  0.1222, -0.0149,  0.1031,  0.0496],\n",
      "         [-0.0547,  0.0099, -0.0441,  0.1292,  0.0531,  0.0397,  0.0345],\n",
      "         [-0.0764,  0.0067, -0.0612,  0.0946,  0.0450,  0.0371,  0.0636],\n",
      "         [-0.0792, -0.0204, -0.0564,  0.1223,  0.0470,  0.0812,  0.0408],\n",
      "         [-0.0306,  0.0504, -0.0977,  0.0816,  0.0662,  0.0725,  0.0322],\n",
      "         [ 0.0045,  0.0107, -0.0630,  0.0807,  0.0002,  0.0588,  0.0407],\n",
      "         [-0.0778,  0.0171, -0.0438,  0.1705,  0.0460,  0.0522,  0.0628],\n",
      "         [-0.0911, -0.0123, -0.0665,  0.1001,  0.0121,  0.0702,  0.0518],\n",
      "         [-0.0019,  0.0138, -0.0629,  0.0777,  0.0824,  0.0393,  0.0399],\n",
      "         [-0.0246, -0.0069, -0.0666,  0.0821, -0.0073,  0.0795,  0.0482],\n",
      "         [-0.0343,  0.0361, -0.0963,  0.0423,  0.0425,  0.1051,  0.0353],\n",
      "         [-0.0029, -0.0013, -0.0494,  0.0698,  0.0587,  0.0487,  0.0449],\n",
      "         [-0.0513,  0.0026, -0.0511,  0.1228,  0.0222,  0.0691,  0.0782],\n",
      "         [-0.0255,  0.0277, -0.0750,  0.1057,  0.0462,  0.0912,  0.0579],\n",
      "         [-0.0178, -0.0161, -0.0692,  0.0852,  0.0150,  0.0622,  0.0456],\n",
      "         [-0.0278, -0.0222, -0.0520,  0.1227,  0.0029,  0.0985,  0.0487],\n",
      "         [-0.0636, -0.0063, -0.0412,  0.1097, -0.0002,  0.0979,  0.0529],\n",
      "         [-0.0504,  0.0275, -0.0641,  0.0690,  0.0174,  0.1119,  0.0555],\n",
      "         [-0.0813, -0.0069, -0.0508,  0.1272,  0.0988,  0.0418,  0.0533],\n",
      "         [-0.0455, -0.0412, -0.0709,  0.1266,  0.0166,  0.0974,  0.0452],\n",
      "         [-0.0270, -0.0040, -0.0727,  0.0697, -0.0137,  0.0661,  0.0376],\n",
      "         [-0.0442, -0.0205, -0.0600,  0.0963,  0.0251,  0.0591,  0.0451],\n",
      "         [-0.0687, -0.0163, -0.0672,  0.0701,  0.0356,  0.0859,  0.0500],\n",
      "         [-0.0561, -0.0027, -0.0817,  0.0493, -0.0096,  0.0911,  0.0397],\n",
      "         [-0.0577,  0.0209, -0.0616,  0.0687,  0.0196,  0.1077,  0.0588],\n",
      "         [-0.0301,  0.0506, -0.0727,  0.0518,  0.0277,  0.1124,  0.0422],\n",
      "         [-0.0155, -0.0191, -0.0500,  0.1078,  0.0073,  0.0889,  0.0512],\n",
      "         [-0.0048,  0.0443, -0.1054,  0.0436,  0.1120,  0.0523,  0.0235],\n",
      "         [-0.0292,  0.0423, -0.0928,  0.0708,  0.0504,  0.0783,  0.0396],\n",
      "         [-0.1034,  0.0109, -0.0629,  0.1239,  0.0457,  0.0162,  0.0741],\n",
      "         [-0.0132,  0.0521, -0.1010,  0.0866, -0.0040,  0.0739,  0.0251],\n",
      "         [-0.0550, -0.0279, -0.0807,  0.1490,  0.0430,  0.0867,  0.0423],\n",
      "         [-0.0230,  0.0402, -0.0643,  0.0855,  0.0815,  0.0565,  0.0379],\n",
      "         [-0.1268, -0.0028, -0.0455,  0.1127,  0.0166,  0.0612,  0.0686],\n",
      "         [-0.0653, -0.0211, -0.0510,  0.1611,  0.0978,  0.0749,  0.0557],\n",
      "         [-0.0839, -0.0256, -0.0725,  0.1207,  0.0357,  0.0576,  0.0752],\n",
      "         [-0.0316, -0.0135, -0.0472,  0.0847, -0.0032,  0.0724,  0.0482],\n",
      "         [-0.0867,  0.0336, -0.0209,  0.1537,  0.0363,  0.0703,  0.0458],\n",
      "         [-0.0322,  0.0458, -0.0907,  0.0682,  0.0610,  0.0708,  0.0309],\n",
      "         [-0.0353,  0.0217, -0.0941,  0.0551,  0.0107,  0.1146,  0.0470],\n",
      "         [ 0.0014,  0.0220, -0.0798,  0.0745, -0.0126,  0.0590,  0.0348],\n",
      "         [-0.0214,  0.0361, -0.1041,  0.0741, -0.0008,  0.0898,  0.0287],\n",
      "         [-0.0350, -0.0035, -0.0635,  0.0755, -0.0051,  0.0660,  0.0432],\n",
      "         [ 0.0054,  0.0214, -0.0655,  0.0688, -0.0051,  0.0577,  0.0368],\n",
      "         [-0.0665,  0.0232, -0.0683,  0.1534,  0.0557,  0.0327,  0.0357],\n",
      "         [-0.0003,  0.0322, -0.0777,  0.0625, -0.0113,  0.0688,  0.0301],\n",
      "         [-0.0705,  0.0186, -0.0317,  0.1407,  0.0364,  0.0887,  0.0468],\n",
      "         [-0.0180,  0.0090, -0.0637,  0.0718,  0.0454,  0.0496,  0.0380],\n",
      "         [ 0.0066,  0.0261, -0.0792,  0.0836,  0.0170,  0.0460,  0.0316],\n",
      "         [-0.0567, -0.0045, -0.0437,  0.1156, -0.0041,  0.1018,  0.0525],\n",
      "         [-0.0055, -0.0042, -0.0554,  0.0772, -0.0048,  0.0631,  0.0492],\n",
      "         [-0.0870,  0.0085, -0.0326,  0.1221,  0.0289,  0.0673,  0.0666],\n",
      "         [ 0.0092,  0.0451, -0.0904,  0.0641,  0.0504,  0.0490,  0.0321],\n",
      "         [-0.0491, -0.0017, -0.0637,  0.0944,  0.0142,  0.0778,  0.0437],\n",
      "         [ 0.0032,  0.0291, -0.0746,  0.0658, -0.0204,  0.0736,  0.0370],\n",
      "         [-0.0074,  0.0211, -0.0680,  0.0568, -0.0154,  0.0670,  0.0350],\n",
      "         [-0.0331, -0.0220, -0.0367,  0.0989,  0.0551,  0.0542,  0.0470],\n",
      "         [-0.0053,  0.0021, -0.0567,  0.0759,  0.0458,  0.0583,  0.0488],\n",
      "         [-0.0455, -0.0077, -0.0679,  0.0940,  0.0476,  0.0242,  0.0612],\n",
      "         [-0.0472,  0.0054, -0.0647,  0.0611,  0.1009,  0.0383,  0.0423],\n",
      "         [-0.0989, -0.0172, -0.0767,  0.1253,  0.0791,  0.0426,  0.0578],\n",
      "         [-0.0707,  0.0074, -0.0410,  0.1251,  0.0325,  0.0836,  0.0549],\n",
      "         [-0.0458,  0.0086, -0.0440,  0.1416,  0.0871,  0.0523,  0.0340],\n",
      "         [-0.0820, -0.0126, -0.0294,  0.1130,  0.0142,  0.0884,  0.0729],\n",
      "         [-0.0179, -0.0189, -0.0748,  0.0907,  0.0143,  0.0680,  0.0466],\n",
      "         [-0.0322,  0.0610, -0.0521,  0.0936,  0.0048,  0.1012,  0.0581],\n",
      "         [ 0.0052,  0.0157, -0.0744,  0.0778, -0.0002,  0.0535,  0.0369],\n",
      "         [ 0.0012,  0.0133, -0.0712,  0.0594, -0.0242,  0.0662,  0.0359],\n",
      "         [ 0.0057,  0.0143, -0.0720,  0.0610, -0.0074,  0.0630,  0.0388],\n",
      "         [-0.0175,  0.0180, -0.0778,  0.0624,  0.1007,  0.0419,  0.0364],\n",
      "         [-0.0648, -0.0221, -0.0524,  0.1172,  0.0722,  0.0520,  0.0469],\n",
      "         [-0.1139,  0.0077, -0.0729,  0.0953,  0.0376,  0.0551,  0.0672],\n",
      "         [-0.0717, -0.0037, -0.0569,  0.0962,  0.0032,  0.0858,  0.0718],\n",
      "         [-0.0376, -0.0056, -0.0826,  0.0774,  0.0127,  0.0750,  0.0491]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), 'pred_boxes': tensor([[[0.4927, 0.4978, 0.4935, 0.5058],\n",
      "         [0.4938, 0.4938, 0.5006, 0.4985],\n",
      "         [0.4978, 0.4982, 0.5020, 0.5056],\n",
      "         [0.4943, 0.5009, 0.5100, 0.5054],\n",
      "         [0.5005, 0.4950, 0.5031, 0.4970],\n",
      "         [0.4904, 0.4975, 0.5037, 0.5108],\n",
      "         [0.4923, 0.4845, 0.5085, 0.5076],\n",
      "         [0.5027, 0.4913, 0.4999, 0.4925],\n",
      "         [0.4903, 0.4940, 0.5117, 0.4981],\n",
      "         [0.5046, 0.4891, 0.5031, 0.4959],\n",
      "         [0.5047, 0.4896, 0.4998, 0.4936],\n",
      "         [0.5106, 0.4884, 0.5110, 0.4948],\n",
      "         [0.4973, 0.5014, 0.5084, 0.5077],\n",
      "         [0.4988, 0.4978, 0.5028, 0.5061],\n",
      "         [0.5019, 0.4912, 0.4912, 0.4911],\n",
      "         [0.4929, 0.4952, 0.5107, 0.4967],\n",
      "         [0.5054, 0.4862, 0.4988, 0.4965],\n",
      "         [0.4938, 0.4949, 0.5091, 0.4989],\n",
      "         [0.5083, 0.4847, 0.4920, 0.4944],\n",
      "         [0.4856, 0.4933, 0.5094, 0.4950],\n",
      "         [0.4932, 0.4969, 0.5046, 0.5050],\n",
      "         [0.4941, 0.4969, 0.5015, 0.5024],\n",
      "         [0.4927, 0.5027, 0.5020, 0.5026],\n",
      "         [0.4977, 0.4905, 0.5048, 0.5006],\n",
      "         [0.4895, 0.4950, 0.5100, 0.4963],\n",
      "         [0.5046, 0.4907, 0.5041, 0.4950],\n",
      "         [0.5026, 0.4925, 0.5094, 0.4957],\n",
      "         [0.5188, 0.5048, 0.5108, 0.4930],\n",
      "         [0.4917, 0.4905, 0.5044, 0.4999],\n",
      "         [0.5085, 0.4997, 0.5069, 0.4943],\n",
      "         [0.4998, 0.4977, 0.4912, 0.4965],\n",
      "         [0.5007, 0.4976, 0.5061, 0.5054],\n",
      "         [0.5087, 0.4968, 0.5160, 0.5034],\n",
      "         [0.4954, 0.4946, 0.5063, 0.4976],\n",
      "         [0.4957, 0.4970, 0.4989, 0.5047],\n",
      "         [0.4955, 0.4936, 0.5102, 0.4957],\n",
      "         [0.4930, 0.5028, 0.5035, 0.5032],\n",
      "         [0.4938, 0.4956, 0.4983, 0.5023],\n",
      "         [0.4997, 0.4963, 0.5060, 0.4991],\n",
      "         [0.5141, 0.5002, 0.5071, 0.5017],\n",
      "         [0.4982, 0.4928, 0.5090, 0.5016],\n",
      "         [0.5064, 0.4895, 0.5092, 0.4944],\n",
      "         [0.4982, 0.4942, 0.5031, 0.4969],\n",
      "         [0.4901, 0.4951, 0.5047, 0.5115],\n",
      "         [0.5096, 0.4864, 0.5010, 0.4890],\n",
      "         [0.5029, 0.4905, 0.5018, 0.4942],\n",
      "         [0.4934, 0.4962, 0.5099, 0.5008],\n",
      "         [0.5038, 0.4870, 0.5029, 0.4991],\n",
      "         [0.4948, 0.4897, 0.5065, 0.5005],\n",
      "         [0.4910, 0.4960, 0.5098, 0.5000],\n",
      "         [0.4940, 0.4907, 0.5052, 0.5065],\n",
      "         [0.4878, 0.4982, 0.5025, 0.5067],\n",
      "         [0.5039, 0.4912, 0.5094, 0.4925],\n",
      "         [0.4919, 0.4981, 0.4931, 0.5048],\n",
      "         [0.5034, 0.4993, 0.4918, 0.4951],\n",
      "         [0.4963, 0.4949, 0.5010, 0.5062],\n",
      "         [0.5039, 0.5009, 0.5095, 0.5073],\n",
      "         [0.5124, 0.5001, 0.5127, 0.4882],\n",
      "         [0.5008, 0.4945, 0.4962, 0.4990],\n",
      "         [0.4981, 0.4910, 0.5094, 0.5010],\n",
      "         [0.5106, 0.4908, 0.5037, 0.4919],\n",
      "         [0.4954, 0.4928, 0.5066, 0.4976],\n",
      "         [0.4934, 0.4932, 0.5108, 0.4932],\n",
      "         [0.5144, 0.4905, 0.5144, 0.4932],\n",
      "         [0.4999, 0.4967, 0.4932, 0.4956],\n",
      "         [0.4938, 0.4955, 0.5062, 0.5047],\n",
      "         [0.5001, 0.4975, 0.5100, 0.5048],\n",
      "         [0.4997, 0.5026, 0.5087, 0.5035],\n",
      "         [0.4897, 0.4968, 0.5112, 0.4960],\n",
      "         [0.5025, 0.4982, 0.5095, 0.5039],\n",
      "         [0.5251, 0.5015, 0.5112, 0.4860],\n",
      "         [0.5006, 0.4988, 0.5118, 0.5002],\n",
      "         [0.5111, 0.4916, 0.5128, 0.4938],\n",
      "         [0.4972, 0.4913, 0.5014, 0.5010],\n",
      "         [0.5034, 0.5012, 0.5061, 0.5094],\n",
      "         [0.4979, 0.4929, 0.5057, 0.4965],\n",
      "         [0.4936, 0.4970, 0.5097, 0.4969],\n",
      "         [0.4978, 0.4943, 0.5098, 0.5108],\n",
      "         [0.4950, 0.4988, 0.5010, 0.5058],\n",
      "         [0.4878, 0.4933, 0.5104, 0.4946],\n",
      "         [0.4974, 0.4989, 0.5122, 0.4980],\n",
      "         [0.4984, 0.4969, 0.5100, 0.5007],\n",
      "         [0.4978, 0.4916, 0.5044, 0.4926],\n",
      "         [0.4972, 0.4964, 0.5017, 0.5038],\n",
      "         [0.5159, 0.4978, 0.5142, 0.4908],\n",
      "         [0.5024, 0.4863, 0.4941, 0.4979],\n",
      "         [0.5081, 0.4862, 0.5062, 0.4899],\n",
      "         [0.5047, 0.4974, 0.5086, 0.5002],\n",
      "         [0.5152, 0.4963, 0.5078, 0.4878],\n",
      "         [0.4974, 0.4941, 0.5040, 0.5058],\n",
      "         [0.5008, 0.4938, 0.5091, 0.5028],\n",
      "         [0.4966, 0.4969, 0.5099, 0.5063],\n",
      "         [0.5005, 0.4969, 0.5068, 0.5071],\n",
      "         [0.4971, 0.4963, 0.5087, 0.5010],\n",
      "         [0.5013, 0.4973, 0.5079, 0.5039],\n",
      "         [0.4923, 0.4928, 0.4931, 0.5016],\n",
      "         [0.5058, 0.4875, 0.4974, 0.4957],\n",
      "         [0.4991, 0.4909, 0.5057, 0.5017],\n",
      "         [0.4921, 0.4939, 0.5021, 0.4980],\n",
      "         [0.4952, 0.4939, 0.5069, 0.5017]]], device='cuda:1',\n",
      "       grad_fn=<SigmoidBackward>)}\n",
      "{'pred_logits': tensor([[[ 9.2123e-03,  2.9666e-02, -9.9994e-02,  5.2890e-02,  8.1584e-02,\n",
      "           7.3284e-02,  2.6715e-02],\n",
      "         [-5.8178e-02, -9.3486e-03, -5.9722e-02,  8.9818e-02,  3.7545e-03,\n",
      "           8.8649e-02,  4.5439e-02],\n",
      "         [ 2.7049e-03,  2.0488e-02, -8.7486e-02,  7.0849e-02,  5.4207e-02,\n",
      "           6.2669e-02,  3.5692e-02],\n",
      "         [-4.4783e-02,  2.2617e-03, -1.0028e-01,  4.6390e-02,  1.3210e-02,\n",
      "           8.6252e-02,  3.0284e-02],\n",
      "         [-9.5623e-02, -3.4232e-03, -7.6489e-02,  9.7530e-02,  3.4767e-02,\n",
      "           5.0286e-02,  6.5787e-02],\n",
      "         [-2.2431e-02,  9.8303e-03, -7.9500e-02,  3.9487e-02,  4.1795e-02,\n",
      "           7.8828e-02,  3.3032e-02],\n",
      "         [-7.6165e-02,  6.6428e-03, -3.8358e-02,  9.2428e-02,  1.7856e-02,\n",
      "           9.0331e-02,  4.6365e-02],\n",
      "         [-3.9162e-02, -2.4602e-02, -4.3078e-02,  1.2320e-01,  7.0796e-02,\n",
      "           6.9385e-02,  5.3674e-02],\n",
      "         [-4.5731e-02, -3.2241e-03, -6.8454e-02,  6.1128e-02, -8.7072e-03,\n",
      "           7.6855e-02,  3.6553e-02],\n",
      "         [-2.4729e-02, -2.9346e-02, -7.2381e-02,  7.3120e-02,  1.6413e-02,\n",
      "           5.8431e-02,  4.5172e-02],\n",
      "         [-6.1100e-02, -2.3527e-02, -5.2022e-02,  1.4656e-01,  7.2307e-02,\n",
      "           6.5517e-02,  6.2840e-02],\n",
      "         [-8.3763e-02, -2.7550e-04, -1.3354e-02,  1.5003e-01,  3.7240e-02,\n",
      "           5.8394e-02,  5.8699e-02],\n",
      "         [-1.2575e-02,  4.0181e-02, -1.1617e-01,  2.1446e-02,  3.2911e-02,\n",
      "           9.1771e-02,  1.3129e-03],\n",
      "         [ 1.3070e-02,  7.1432e-04, -5.1783e-02,  6.7494e-02,  1.9989e-02,\n",
      "           5.5125e-02,  5.4889e-02],\n",
      "         [-4.8277e-02, -7.9774e-04, -7.0881e-02,  8.0960e-02,  8.4157e-02,\n",
      "           1.7032e-02,  5.6203e-02],\n",
      "         [-3.1508e-02, -1.8391e-02, -5.5598e-02,  7.9159e-02, -1.9952e-03,\n",
      "           7.0977e-02,  5.3437e-02],\n",
      "         [-5.3342e-02,  2.8270e-03, -6.6757e-02,  7.4177e-02,  8.4933e-02,\n",
      "           5.1200e-02,  3.9881e-02],\n",
      "         [-3.0169e-02, -3.1599e-02, -7.1516e-02,  7.3036e-02, -8.3387e-03,\n",
      "           5.7242e-02,  4.4790e-02],\n",
      "         [-4.3840e-02,  1.5197e-02, -1.9677e-02,  9.8548e-02,  9.2157e-02,\n",
      "           2.8010e-02,  4.6286e-02],\n",
      "         [-5.5547e-02, -6.4218e-03, -6.9398e-02,  5.6833e-02, -6.8453e-03,\n",
      "           8.3909e-02,  4.0045e-02],\n",
      "         [-2.9132e-02,  5.4305e-03, -7.2489e-02,  5.2003e-02,  3.8274e-02,\n",
      "           7.5629e-02,  3.5279e-02],\n",
      "         [-4.3687e-02,  9.9030e-03, -6.8010e-02,  4.9395e-02,  2.9571e-02,\n",
      "           7.5966e-02,  4.2412e-02],\n",
      "         [-2.7317e-02,  2.0190e-02, -7.9842e-02,  3.5426e-02,  3.0894e-02,\n",
      "           9.6292e-02,  2.3682e-02],\n",
      "         [-1.0445e-01, -1.3499e-02, -5.7717e-02,  1.1765e-01,  4.3643e-02,\n",
      "           3.5794e-02,  8.0266e-02],\n",
      "         [-4.3716e-02, -1.6477e-02, -6.3890e-02,  7.1229e-02,  9.4609e-03,\n",
      "           5.8661e-02,  3.9254e-02],\n",
      "         [-2.6664e-02, -3.1912e-02, -3.9746e-02,  1.0018e-01,  3.7353e-02,\n",
      "           6.9902e-02,  5.5516e-02],\n",
      "         [-2.8145e-02, -1.7939e-02, -5.4762e-02,  7.3549e-02, -4.6060e-03,\n",
      "           7.7389e-02,  5.1254e-02],\n",
      "         [-5.9129e-02, -9.3349e-03, -3.9500e-02,  1.5229e-01,  7.8076e-02,\n",
      "           5.0909e-02,  4.7332e-02],\n",
      "         [-6.5545e-02,  1.5150e-03, -4.7676e-02,  6.3770e-02,  2.9487e-02,\n",
      "           1.0266e-01,  5.1600e-02],\n",
      "         [-6.8688e-02, -2.5529e-02, -6.6267e-02,  1.0834e-01, -2.3329e-03,\n",
      "           9.8003e-02,  5.7302e-02],\n",
      "         [-2.6500e-02, -5.1364e-03, -1.0405e-01,  5.0667e-02,  4.9762e-02,\n",
      "           5.8827e-02,  4.2046e-02],\n",
      "         [ 4.4304e-03,  6.9331e-03, -5.3497e-02,  9.0502e-02,  1.5285e-02,\n",
      "           6.5177e-02,  5.8553e-02],\n",
      "         [-8.2920e-02, -3.9076e-02, -7.4933e-02,  1.4500e-01,  7.6976e-02,\n",
      "           6.3461e-02,  7.8306e-02],\n",
      "         [-4.4253e-02, -6.7836e-03, -6.8974e-02,  7.5469e-02,  8.5349e-03,\n",
      "           6.9768e-02,  4.0050e-02],\n",
      "         [ 9.8756e-03,  2.7209e-02, -8.8657e-02,  6.9588e-02,  7.1236e-02,\n",
      "           7.4001e-02,  3.0444e-02],\n",
      "         [-1.8401e-02, -2.6230e-02, -5.6132e-02,  7.9514e-02, -3.8937e-03,\n",
      "           8.5034e-02,  5.3053e-02],\n",
      "         [-4.4510e-02,  2.7539e-02, -9.2083e-02,  5.2630e-02,  2.1144e-02,\n",
      "           7.7866e-02,  2.2600e-02],\n",
      "         [-4.8886e-03, -1.9859e-03, -4.8122e-02,  7.2996e-02,  4.7751e-02,\n",
      "           5.7022e-02,  4.3637e-02],\n",
      "         [-6.7451e-02,  2.5852e-02, -4.9657e-02,  6.6520e-02,  1.8906e-02,\n",
      "           1.0040e-01,  4.2488e-02],\n",
      "         [-3.3672e-02,  2.3552e-02, -9.6895e-02,  1.0593e-01,  1.0513e-01,\n",
      "           6.6459e-02,  5.3481e-02],\n",
      "         [-2.6189e-02, -4.3064e-02, -5.9311e-02,  8.6013e-02, -6.6646e-03,\n",
      "           5.5684e-02,  5.4321e-02],\n",
      "         [-3.1708e-02, -3.8023e-02, -7.1512e-02,  1.3152e-01,  1.1636e-02,\n",
      "           1.0238e-01,  4.8668e-02],\n",
      "         [-6.0677e-02, -1.9624e-02, -6.1192e-02,  1.1390e-01,  2.7958e-03,\n",
      "           1.0580e-01,  5.0465e-02],\n",
      "         [-5.0377e-02,  1.9691e-02, -7.3143e-02,  7.5179e-02,  2.8131e-02,\n",
      "           9.6869e-02,  5.1786e-02],\n",
      "         [-7.1180e-02, -1.5960e-02, -4.5645e-02,  1.4821e-01,  8.2337e-02,\n",
      "           4.1277e-02,  6.9323e-02],\n",
      "         [-3.3670e-02, -4.1962e-02, -6.2601e-02,  1.1657e-01,  3.6097e-02,\n",
      "           8.4795e-02,  5.3895e-02],\n",
      "         [-1.7245e-02, -1.5920e-02, -6.5265e-02,  6.2083e-02, -1.7275e-02,\n",
      "           5.7439e-02,  5.0262e-02],\n",
      "         [-3.8960e-02, -2.6378e-02, -5.8875e-02,  8.9100e-02,  2.4612e-02,\n",
      "           5.4409e-02,  5.6951e-02],\n",
      "         [-6.5318e-02,  2.8282e-04, -6.3302e-02,  5.9666e-02,  6.4828e-03,\n",
      "           8.7531e-02,  3.8242e-02],\n",
      "         [-2.9701e-02,  1.9350e-02, -6.9669e-02,  5.3467e-02,  5.8324e-03,\n",
      "           7.2614e-02,  3.5779e-02],\n",
      "         [-7.5829e-02, -1.6409e-03, -6.4353e-02,  5.9803e-02,  1.0879e-02,\n",
      "           9.3583e-02,  3.5496e-02],\n",
      "         [-4.1612e-02,  1.7441e-02, -8.3712e-02,  4.5499e-02,  1.7368e-02,\n",
      "           9.5707e-02,  3.6990e-02],\n",
      "         [-1.7242e-02, -3.3060e-02, -5.3464e-02,  9.4574e-02,  1.2268e-02,\n",
      "           7.8535e-02,  5.0242e-02],\n",
      "         [-4.4838e-03,  3.8850e-02, -9.7469e-02,  3.4238e-02,  1.0377e-01,\n",
      "           5.9624e-02,  2.2682e-02],\n",
      "         [-4.4522e-02, -8.2535e-03, -1.0537e-01,  8.8980e-02,  2.6219e-02,\n",
      "           4.0109e-02,  6.7868e-02],\n",
      "         [-1.2153e-01, -1.6368e-02, -6.2344e-02,  1.2917e-01,  3.0149e-02,\n",
      "           2.8735e-02,  8.3694e-02],\n",
      "         [ 1.0657e-02,  4.1442e-02, -1.1667e-01,  3.9331e-02,  2.1963e-02,\n",
      "           8.9692e-02,  1.5134e-02],\n",
      "         [-5.2784e-02, -4.0600e-02, -7.6567e-02,  1.3756e-01,  6.5156e-02,\n",
      "           9.1632e-02,  3.9882e-02],\n",
      "         [-1.9206e-02,  7.9333e-03, -8.1780e-02,  6.1043e-02,  8.0429e-02,\n",
      "           5.2558e-02,  3.1475e-02],\n",
      "         [-1.0925e-01, -4.5490e-03, -3.4541e-02,  1.0487e-01,  1.5556e-02,\n",
      "           6.1042e-02,  5.1367e-02],\n",
      "         [-6.4663e-02, -2.3505e-02, -4.8662e-02,  1.4830e-01,  7.5796e-02,\n",
      "           7.0911e-02,  6.1533e-02],\n",
      "         [-6.0956e-02, -7.8534e-03, -5.1903e-02,  6.9792e-02,  2.8811e-02,\n",
      "           8.8823e-02,  4.4911e-02],\n",
      "         [-2.8488e-02, -2.6412e-02, -5.5902e-02,  7.9728e-02, -5.5157e-03,\n",
      "           7.3032e-02,  5.3563e-02],\n",
      "         [-8.2033e-02, -2.0013e-02, -4.4349e-02,  1.4698e-01,  4.8752e-02,\n",
      "           8.5844e-02,  6.6902e-02],\n",
      "         [-2.8776e-02,  1.1432e-02, -1.0007e-01,  4.0528e-02,  1.3606e-02,\n",
      "           9.8324e-02,  1.5531e-02],\n",
      "         [-5.3974e-02,  1.1403e-02, -7.1501e-02,  6.0613e-02,  1.0077e-02,\n",
      "           7.9644e-02,  3.7172e-02],\n",
      "         [-3.1696e-03,  4.2518e-02, -1.0552e-01,  6.1478e-02,  1.0252e-02,\n",
      "           8.4104e-02,  2.1474e-02],\n",
      "         [-3.0182e-02,  3.6271e-02, -7.5282e-02,  5.3199e-02,  7.8840e-03,\n",
      "           7.5633e-02,  2.4749e-02],\n",
      "         [-3.6870e-02,  1.6130e-02, -7.4779e-02,  6.0399e-02,  7.8558e-04,\n",
      "           6.7234e-02,  3.6398e-02],\n",
      "         [-4.6603e-03,  2.7839e-02, -7.1811e-02,  7.9588e-02, -2.2257e-02,\n",
      "           7.8140e-02,  4.1646e-02],\n",
      "         [-7.5433e-02, -3.9577e-02, -7.2217e-02,  1.4158e-01,  8.2461e-02,\n",
      "           5.4485e-02,  4.4037e-02],\n",
      "         [-1.1117e-02,  2.3808e-02, -6.6050e-02,  6.6207e-02, -3.2851e-02,\n",
      "           7.5951e-02,  4.8564e-02],\n",
      "         [-9.0966e-02,  1.2556e-02, -1.2150e-02,  1.5002e-01,  4.2657e-02,\n",
      "           6.0148e-02,  4.6395e-02],\n",
      "         [ 7.8587e-03, -6.1688e-03, -5.0777e-02,  6.7965e-02,  2.2273e-02,\n",
      "           5.6759e-02,  5.6070e-02],\n",
      "         [ 1.3166e-02,  3.0131e-02, -8.4265e-02,  8.4179e-02,  2.3072e-02,\n",
      "           6.4697e-02,  4.7578e-02],\n",
      "         [-5.2667e-02, -2.1253e-02, -6.6715e-02,  1.1574e-01, -1.6864e-03,\n",
      "           1.1412e-01,  4.6926e-02],\n",
      "         [-1.5267e-02, -2.4622e-02, -5.8257e-02,  7.7081e-02, -2.3041e-03,\n",
      "           5.3403e-02,  5.3495e-02],\n",
      "         [-8.4441e-02, -1.1731e-02, -2.5544e-02,  1.3798e-01,  4.8413e-02,\n",
      "           4.4603e-02,  7.0565e-02],\n",
      "         [ 1.1097e-02,  3.2011e-02, -8.3273e-02,  8.3616e-02,  4.4095e-02,\n",
      "           7.1371e-02,  3.8786e-02],\n",
      "         [-4.1641e-02, -1.0115e-03, -6.5096e-02,  7.4145e-02,  1.6103e-02,\n",
      "           8.9964e-02,  6.8703e-02],\n",
      "         [-2.0500e-02,  2.6277e-02, -6.6863e-02,  6.4311e-02, -2.6000e-02,\n",
      "           7.6439e-02,  3.6565e-02],\n",
      "         [-2.0468e-02,  1.3142e-02, -6.4164e-02,  6.3746e-02, -1.1937e-02,\n",
      "           6.7841e-02,  3.6179e-02],\n",
      "         [-4.0280e-02,  7.3880e-03, -3.7896e-02,  6.3206e-02,  9.5068e-02,\n",
      "           3.6854e-02,  3.9155e-02],\n",
      "         [-7.4439e-03,  1.1227e-04, -5.4093e-02,  7.7445e-02,  5.3774e-02,\n",
      "           5.0873e-02,  5.0096e-02],\n",
      "         [-4.0284e-02, -5.1685e-03, -6.6741e-02,  9.1706e-02,  3.9372e-02,\n",
      "           3.0965e-02,  5.0124e-02],\n",
      "         [-3.9792e-02, -1.9086e-03, -5.7089e-02,  5.9271e-02,  9.2557e-02,\n",
      "           3.0436e-02,  4.5420e-02],\n",
      "         [-5.2566e-02, -5.6785e-03, -6.0836e-02,  8.7471e-02,  7.7935e-02,\n",
      "           4.8701e-02,  4.5719e-02],\n",
      "         [-6.5103e-02, -1.0750e-02, -4.0739e-02,  1.3708e-01,  4.8090e-02,\n",
      "           8.2915e-02,  5.7595e-02],\n",
      "         [-5.3464e-02, -4.6491e-03, -3.6692e-02,  1.4331e-01,  8.3903e-02,\n",
      "           5.7534e-02,  3.8146e-02],\n",
      "         [-7.2398e-02, -3.3653e-02, -4.1177e-02,  1.2845e-01,  2.5452e-02,\n",
      "           7.6169e-02,  7.5380e-02],\n",
      "         [-3.9492e-02, -1.8328e-02, -7.3227e-02,  8.7182e-02,  2.1917e-02,\n",
      "           5.3431e-02,  4.7734e-02],\n",
      "         [-1.9755e-02,  5.9842e-02, -6.0638e-02,  7.5532e-02,  2.7002e-02,\n",
      "           8.7186e-02,  5.0734e-02],\n",
      "         [-1.1315e-02,  1.5446e-02, -7.4949e-02,  7.9902e-02, -1.4326e-02,\n",
      "           6.5620e-02,  2.9194e-02],\n",
      "         [-8.4139e-03,  3.6236e-03, -6.9721e-02,  6.5494e-02, -2.1537e-02,\n",
      "           6.0065e-02,  3.9813e-02],\n",
      "         [-3.1265e-02, -2.0253e-02, -6.8755e-02,  9.6657e-02, -6.7743e-03,\n",
      "           6.6034e-02,  4.0568e-02],\n",
      "         [-1.7834e-02,  3.0464e-02, -8.3540e-02,  3.9913e-02,  1.0157e-01,\n",
      "           6.4628e-02,  2.3386e-02],\n",
      "         [-3.2093e-02, -3.3534e-02, -4.6086e-02,  8.6609e-02,  7.1522e-02,\n",
      "           3.6189e-02,  4.5983e-02],\n",
      "         [-9.6958e-02, -8.7807e-03, -7.1079e-02,  1.0233e-01,  3.7206e-02,\n",
      "           4.9939e-02,  6.6528e-02],\n",
      "         [-7.1929e-02, -2.8648e-02, -4.4319e-02,  9.2300e-02,  2.5114e-02,\n",
      "           7.3538e-02,  5.6172e-02],\n",
      "         [-3.5264e-02, -4.9426e-03, -9.1377e-02,  5.9017e-02,  2.4838e-02,\n",
      "           7.9104e-02,  3.6897e-02]]], device='cuda:1', grad_fn=<AddBackward0>), 'pred_boxes': tensor([[[0.4946, 0.5016, 0.4977, 0.5110],\n",
      "         [0.4902, 0.4948, 0.5068, 0.4961],\n",
      "         [0.4959, 0.5022, 0.5025, 0.5124],\n",
      "         [0.4921, 0.5043, 0.5023, 0.5027],\n",
      "         [0.4982, 0.4940, 0.5029, 0.4997],\n",
      "         [0.4906, 0.5008, 0.5054, 0.5050],\n",
      "         [0.4942, 0.4873, 0.5099, 0.5057],\n",
      "         [0.5027, 0.4931, 0.5002, 0.4882],\n",
      "         [0.4886, 0.4956, 0.5077, 0.4973],\n",
      "         [0.4974, 0.4904, 0.5032, 0.4989],\n",
      "         [0.5124, 0.4908, 0.4952, 0.4934],\n",
      "         [0.5136, 0.4921, 0.5109, 0.4906],\n",
      "         [0.4908, 0.5077, 0.5004, 0.5017],\n",
      "         [0.4967, 0.4956, 0.5030, 0.5030],\n",
      "         [0.5068, 0.4874, 0.4892, 0.4925],\n",
      "         [0.4907, 0.4958, 0.5065, 0.4947],\n",
      "         [0.5015, 0.4848, 0.4996, 0.5004],\n",
      "         [0.4946, 0.4920, 0.5061, 0.4970],\n",
      "         [0.5088, 0.4877, 0.4888, 0.4919],\n",
      "         [0.4830, 0.4953, 0.5076, 0.4970],\n",
      "         [0.4918, 0.5032, 0.5051, 0.5042],\n",
      "         [0.4915, 0.5019, 0.5048, 0.5015],\n",
      "         [0.4918, 0.5064, 0.5030, 0.5016],\n",
      "         [0.4992, 0.4905, 0.5042, 0.5025],\n",
      "         [0.4899, 0.4955, 0.5052, 0.4947],\n",
      "         [0.5061, 0.4934, 0.5000, 0.4950],\n",
      "         [0.4897, 0.4959, 0.5060, 0.4959],\n",
      "         [0.5209, 0.5045, 0.5102, 0.4890],\n",
      "         [0.4896, 0.4894, 0.5092, 0.5079],\n",
      "         [0.4980, 0.4927, 0.5050, 0.4986],\n",
      "         [0.4977, 0.4933, 0.4954, 0.5032],\n",
      "         [0.5013, 0.4954, 0.5060, 0.5065],\n",
      "         [0.5153, 0.4917, 0.5032, 0.4841],\n",
      "         [0.4895, 0.4960, 0.5070, 0.4950],\n",
      "         [0.4949, 0.5010, 0.4986, 0.5121],\n",
      "         [0.4959, 0.4937, 0.5064, 0.4962],\n",
      "         [0.4948, 0.4998, 0.5070, 0.5016],\n",
      "         [0.4996, 0.4951, 0.5014, 0.5044],\n",
      "         [0.4897, 0.4874, 0.5086, 0.5119],\n",
      "         [0.5027, 0.4931, 0.5010, 0.5017],\n",
      "         [0.4965, 0.4918, 0.5061, 0.4965],\n",
      "         [0.5073, 0.4913, 0.5052, 0.4938],\n",
      "         [0.4998, 0.4957, 0.5050, 0.4942],\n",
      "         [0.4896, 0.4964, 0.5062, 0.5088],\n",
      "         [0.5145, 0.4913, 0.4977, 0.4909],\n",
      "         [0.5057, 0.4924, 0.4986, 0.4942],\n",
      "         [0.4922, 0.4965, 0.5048, 0.4968],\n",
      "         [0.5026, 0.4905, 0.5006, 0.5001],\n",
      "         [0.4853, 0.4935, 0.5082, 0.4990],\n",
      "         [0.4963, 0.4998, 0.5070, 0.5008],\n",
      "         [0.4893, 0.4889, 0.5091, 0.5013],\n",
      "         [0.4872, 0.4979, 0.5088, 0.5042],\n",
      "         [0.5004, 0.4913, 0.5031, 0.4971],\n",
      "         [0.4918, 0.4941, 0.4932, 0.5065],\n",
      "         [0.5013, 0.4951, 0.4914, 0.4946],\n",
      "         [0.4986, 0.4902, 0.5025, 0.5010],\n",
      "         [0.4937, 0.5050, 0.5058, 0.5035],\n",
      "         [0.5125, 0.4991, 0.5145, 0.4918],\n",
      "         [0.4943, 0.4913, 0.4924, 0.5017],\n",
      "         [0.5012, 0.4850, 0.5066, 0.4988],\n",
      "         [0.5138, 0.4908, 0.5022, 0.4878],\n",
      "         [0.4897, 0.4874, 0.5095, 0.5038],\n",
      "         [0.4911, 0.4950, 0.5062, 0.4945],\n",
      "         [0.5138, 0.4926, 0.5087, 0.4870],\n",
      "         [0.4902, 0.5033, 0.5035, 0.5014],\n",
      "         [0.4888, 0.4965, 0.5075, 0.4984],\n",
      "         [0.4971, 0.5004, 0.5060, 0.4974],\n",
      "         [0.4995, 0.4997, 0.5057, 0.5022],\n",
      "         [0.4960, 0.4959, 0.5071, 0.4977],\n",
      "         [0.4988, 0.5012, 0.5084, 0.5054],\n",
      "         [0.5218, 0.5016, 0.5098, 0.4861],\n",
      "         [0.4956, 0.4981, 0.5073, 0.5019],\n",
      "         [0.5144, 0.4943, 0.5117, 0.4916],\n",
      "         [0.4984, 0.4933, 0.5035, 0.5004],\n",
      "         [0.4966, 0.5038, 0.5023, 0.5095],\n",
      "         [0.5007, 0.4941, 0.5066, 0.4958],\n",
      "         [0.4928, 0.4972, 0.5059, 0.4958],\n",
      "         [0.5076, 0.4946, 0.5097, 0.5021],\n",
      "         [0.4962, 0.5021, 0.5007, 0.5116],\n",
      "         [0.4884, 0.5054, 0.5148, 0.4924],\n",
      "         [0.5027, 0.4976, 0.5102, 0.4988],\n",
      "         [0.5016, 0.4967, 0.5075, 0.4998],\n",
      "         [0.5043, 0.4861, 0.4945, 0.4920],\n",
      "         [0.4986, 0.4968, 0.5005, 0.5070],\n",
      "         [0.5154, 0.4963, 0.5127, 0.4909],\n",
      "         [0.4988, 0.4875, 0.4919, 0.4968],\n",
      "         [0.5005, 0.4867, 0.5009, 0.4930],\n",
      "         [0.5088, 0.5016, 0.5085, 0.4989],\n",
      "         [0.5158, 0.4961, 0.5019, 0.4934],\n",
      "         [0.5019, 0.4918, 0.5050, 0.5003],\n",
      "         [0.5015, 0.4897, 0.5045, 0.5026],\n",
      "         [0.4922, 0.4983, 0.5100, 0.5035],\n",
      "         [0.5029, 0.4958, 0.5081, 0.5036],\n",
      "         [0.4984, 0.4949, 0.5069, 0.5013],\n",
      "         [0.5016, 0.4925, 0.5066, 0.5030],\n",
      "         [0.4937, 0.4915, 0.4964, 0.5055],\n",
      "         [0.5043, 0.4919, 0.4930, 0.4929],\n",
      "         [0.4993, 0.4924, 0.5062, 0.5007],\n",
      "         [0.4926, 0.4907, 0.5075, 0.4953],\n",
      "         [0.4925, 0.5032, 0.5059, 0.4965]]], device='cuda:1',\n",
      "       grad_fn=<SigmoidBackward>)}\n",
      "{'pred_logits': tensor([[[-0.0244,  0.0225, -0.1137,  0.0468,  0.0991,  0.0365,  0.0232],\n",
      "         [-0.0728, -0.0016, -0.0607,  0.0917,  0.0150,  0.0917,  0.0406],\n",
      "         [-0.0004,  0.0211, -0.0759,  0.0755,  0.0497,  0.0448,  0.0403],\n",
      "         [-0.0394,  0.0081, -0.0883,  0.0675,  0.0188,  0.0969,  0.0308],\n",
      "         [-0.0913, -0.0095, -0.0808,  0.0908,  0.0367,  0.0512,  0.0692],\n",
      "         [-0.0429,  0.0403, -0.0820,  0.0324,  0.0163,  0.0988,  0.0272],\n",
      "         [-0.0724,  0.0146, -0.0534,  0.0834,  0.0171,  0.0803,  0.0413],\n",
      "         [-0.0734, -0.0171, -0.0617,  0.0892,  0.0937,  0.0445,  0.0587],\n",
      "         [-0.0486,  0.0127, -0.0796,  0.0536,  0.0138,  0.0759,  0.0351],\n",
      "         [-0.0218, -0.0135, -0.0549,  0.0792,  0.0549,  0.0468,  0.0413],\n",
      "         [-0.0451, -0.0006, -0.0570,  0.1365,  0.0900,  0.0372,  0.0442],\n",
      "         [-0.0764, -0.0175, -0.0450,  0.1479,  0.0560,  0.0718,  0.0675],\n",
      "         [-0.0353,  0.0140, -0.1091,  0.0400,  0.0032,  0.0894,  0.0228],\n",
      "         [ 0.0014,  0.0061, -0.0581,  0.0755,  0.0183,  0.0487,  0.0432],\n",
      "         [-0.0549, -0.0075, -0.0679,  0.0628,  0.0943,  0.0172,  0.0487],\n",
      "         [-0.0474, -0.0207, -0.0590,  0.0918, -0.0059,  0.0808,  0.0511],\n",
      "         [-0.0595,  0.0113, -0.0877,  0.0713,  0.0953,  0.0458,  0.0488],\n",
      "         [-0.0438, -0.0300, -0.0699,  0.0913, -0.0181,  0.0716,  0.0477],\n",
      "         [-0.0652, -0.0006, -0.0673,  0.1001,  0.0725,  0.0427,  0.0476],\n",
      "         [-0.0713, -0.0169, -0.0835,  0.0585,  0.0168,  0.0992,  0.0402],\n",
      "         [-0.0405,  0.0179, -0.0852,  0.0487,  0.0062,  0.0859,  0.0283],\n",
      "         [-0.0590,  0.0238, -0.0747,  0.0542,  0.0173,  0.0754,  0.0348],\n",
      "         [-0.0326,  0.0290, -0.0864,  0.0391,  0.0111,  0.0861,  0.0273],\n",
      "         [-0.0775, -0.0167, -0.0474,  0.1098,  0.0430,  0.0606,  0.0712],\n",
      "         [-0.0656, -0.0145, -0.0656,  0.0893,  0.0053,  0.0832,  0.0471],\n",
      "         [-0.0287, -0.0025, -0.0503,  0.0834,  0.0574,  0.0554,  0.0510],\n",
      "         [-0.0507, -0.0180, -0.0592,  0.0870, -0.0033,  0.0844,  0.0532],\n",
      "         [-0.0630, -0.0264, -0.0574,  0.1090,  0.0695,  0.0677,  0.0453],\n",
      "         [-0.0630,  0.0044, -0.0602,  0.0699,  0.0129,  0.0999,  0.0466],\n",
      "         [-0.0680, -0.0167, -0.0350,  0.1201,  0.0438,  0.0717,  0.0525],\n",
      "         [-0.0406,  0.0163, -0.0915,  0.0704,  0.0373,  0.0583,  0.0302],\n",
      "         [ 0.0144,  0.0378, -0.0979,  0.0776,  0.0441,  0.0718,  0.0359],\n",
      "         [-0.0904, -0.0011, -0.0625,  0.1575,  0.0598,  0.0468,  0.0682],\n",
      "         [-0.0690, -0.0147, -0.0663,  0.0958,  0.0156,  0.0830,  0.0480],\n",
      "         [-0.0165,  0.0196, -0.0718,  0.0691,  0.0712,  0.0214,  0.0438],\n",
      "         [-0.0317,  0.0038, -0.0610,  0.1091,  0.0158,  0.0779,  0.0539],\n",
      "         [-0.0316,  0.0119, -0.0884,  0.0619,  0.0407,  0.0834,  0.0405],\n",
      "         [-0.0037, -0.0069, -0.0598,  0.0687,  0.0578,  0.0520,  0.0532],\n",
      "         [-0.0610,  0.0337, -0.0668,  0.0837, -0.0023,  0.0904,  0.0397],\n",
      "         [-0.0243,  0.0358, -0.0514,  0.0758,  0.0857,  0.0672,  0.0492],\n",
      "         [-0.0204, -0.0321, -0.0595,  0.0813,  0.0085,  0.0519,  0.0514],\n",
      "         [-0.0416, -0.0133, -0.0614,  0.1002,  0.0266,  0.0836,  0.0567],\n",
      "         [-0.0655, -0.0054, -0.0633,  0.0986,  0.0154,  0.0905,  0.0467],\n",
      "         [-0.0425,  0.0496, -0.0773,  0.0400,  0.0141,  0.1006,  0.0268],\n",
      "         [-0.0645, -0.0073, -0.0528,  0.1207,  0.0920,  0.0538,  0.0615],\n",
      "         [-0.0298, -0.0172, -0.0634,  0.1151,  0.0527,  0.0751,  0.0476],\n",
      "         [-0.0183,  0.0088, -0.0761,  0.0664, -0.0184,  0.0766,  0.0349],\n",
      "         [-0.0226, -0.0329, -0.0625,  0.0847,  0.0282,  0.0557,  0.0441],\n",
      "         [-0.0764, -0.0011, -0.0743,  0.0670,  0.0266,  0.0984,  0.0297],\n",
      "         [-0.0540,  0.0212, -0.0867,  0.0681,  0.0135,  0.0804,  0.0337],\n",
      "         [-0.0246,  0.0401, -0.0687,  0.0567,  0.0411,  0.1005,  0.0417],\n",
      "         [-0.0211,  0.0397, -0.0817,  0.0347,  0.0239,  0.0970,  0.0340],\n",
      "         [-0.0222, -0.0094, -0.0433,  0.0913,  0.0421,  0.0675,  0.0554],\n",
      "         [-0.0042,  0.0152, -0.0755,  0.0669,  0.0823,  0.0396,  0.0424],\n",
      "         [-0.0406,  0.0029, -0.1097,  0.0637,  0.0662,  0.0412,  0.0400],\n",
      "         [-0.1253, -0.0168, -0.0612,  0.1222,  0.0365,  0.0389,  0.0686],\n",
      "         [-0.0080,  0.0372, -0.1381,  0.0180,  0.0390,  0.0648,  0.0106],\n",
      "         [-0.0430, -0.0073, -0.0669,  0.1209,  0.0401,  0.0796,  0.0328],\n",
      "         [-0.0293,  0.0157, -0.0917,  0.0523,  0.0962,  0.0491,  0.0373],\n",
      "         [-0.1106, -0.0108, -0.0472,  0.1068,  0.0244,  0.0540,  0.0535],\n",
      "         [-0.0867, -0.0222, -0.0654,  0.1389,  0.0771,  0.0571,  0.0565],\n",
      "         [-0.0703, -0.0092, -0.0484,  0.0683,  0.0367,  0.0796,  0.0425],\n",
      "         [-0.0282, -0.0249, -0.0629,  0.1002,  0.0090,  0.0764,  0.0573],\n",
      "         [-0.1086,  0.0056, -0.0204,  0.1345,  0.0454,  0.0603,  0.0427],\n",
      "         [-0.0385,  0.0142, -0.1089,  0.0398,  0.0342,  0.0485,  0.0232],\n",
      "         [-0.0614,  0.0172, -0.0844,  0.0426,  0.0185,  0.0746,  0.0261],\n",
      "         [-0.0337,  0.0205, -0.0916,  0.0781,  0.0250,  0.0726,  0.0368],\n",
      "         [-0.0363,  0.0303, -0.1032,  0.0475,  0.0191,  0.0872,  0.0337],\n",
      "         [-0.0454,  0.0143, -0.1006,  0.0635, -0.0016,  0.0900,  0.0299],\n",
      "         [-0.0133,  0.0155, -0.0872,  0.0768,  0.0118,  0.0337,  0.0298],\n",
      "         [-0.0595, -0.0161, -0.0501,  0.1361,  0.0461,  0.0654,  0.0410],\n",
      "         [-0.0071,  0.0211, -0.0851,  0.0681,  0.0037,  0.0442,  0.0382],\n",
      "         [-0.0494, -0.0052, -0.0462,  0.1431,  0.0890,  0.0809,  0.0463],\n",
      "         [-0.0045, -0.0106, -0.0657,  0.0831,  0.0273,  0.0530,  0.0374],\n",
      "         [ 0.0056,  0.0182, -0.0732,  0.0778,  0.0167,  0.0449,  0.0454],\n",
      "         [-0.0603, -0.0161, -0.0743,  0.1135,  0.0258,  0.0835,  0.0367],\n",
      "         [-0.0400, -0.0090, -0.0791,  0.0892, -0.0338,  0.0686,  0.0391],\n",
      "         [-0.0994, -0.0110, -0.0429,  0.1170,  0.0472,  0.0474,  0.0515],\n",
      "         [-0.0060,  0.0118, -0.1157,  0.0444,  0.0508,  0.0448,  0.0339],\n",
      "         [-0.0409,  0.0267, -0.0956,  0.0512,  0.0488,  0.1096,  0.0192],\n",
      "         [-0.0123,  0.0405, -0.0959,  0.0507,  0.0030,  0.0711,  0.0293],\n",
      "         [-0.0127,  0.0355, -0.0944,  0.0488, -0.0027,  0.0684,  0.0296],\n",
      "         [-0.0483,  0.0088, -0.0506,  0.0714,  0.0922,  0.0448,  0.0481],\n",
      "         [-0.0038, -0.0121, -0.0542,  0.0727,  0.0383,  0.0530,  0.0537],\n",
      "         [-0.0547, -0.0097, -0.0792,  0.0786,  0.0450,  0.0299,  0.0512],\n",
      "         [-0.0201, -0.0012, -0.0709,  0.0629,  0.0961,  0.0259,  0.0551],\n",
      "         [-0.0409, -0.0088, -0.0867,  0.0767,  0.0924,  0.0483,  0.0529],\n",
      "         [-0.0625, -0.0127, -0.0427,  0.1169,  0.0466,  0.0779,  0.0506],\n",
      "         [-0.0470,  0.0018, -0.0561,  0.1292,  0.0821,  0.0540,  0.0421],\n",
      "         [-0.0757, -0.0158, -0.0494,  0.1021,  0.0216,  0.0811,  0.0673],\n",
      "         [-0.0075, -0.0165, -0.0672,  0.0668,  0.0029,  0.0528,  0.0482],\n",
      "         [-0.0594,  0.0430, -0.0300,  0.0852, -0.0010,  0.0818,  0.0352],\n",
      "         [ 0.0048,  0.0139, -0.0574,  0.0776, -0.0016,  0.0465,  0.0451],\n",
      "         [ 0.0040,  0.0081, -0.0682,  0.0714, -0.0115,  0.0599,  0.0484],\n",
      "         [-0.0084, -0.0033, -0.0622,  0.0770, -0.0110,  0.0628,  0.0490],\n",
      "         [-0.0198, -0.0021, -0.0582,  0.0627,  0.1009,  0.0388,  0.0530],\n",
      "         [-0.0596, -0.0182, -0.0605,  0.0962,  0.0795,  0.0488,  0.0551],\n",
      "         [-0.1078, -0.0135, -0.0812,  0.0976,  0.0309,  0.0499,  0.0682],\n",
      "         [-0.0652, -0.0139, -0.0616,  0.0950,  0.0185,  0.0833,  0.0658],\n",
      "         [-0.0490, -0.0014, -0.1060,  0.0717,  0.0208,  0.0751,  0.0547]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>), 'pred_boxes': tensor([[[0.4907, 0.4927, 0.4930, 0.5022],\n",
      "         [0.4930, 0.4937, 0.5007, 0.4953],\n",
      "         [0.4931, 0.4991, 0.4989, 0.5041],\n",
      "         [0.4924, 0.5051, 0.5045, 0.4996],\n",
      "         [0.4987, 0.4938, 0.5029, 0.4965],\n",
      "         [0.4846, 0.4988, 0.5087, 0.5128],\n",
      "         [0.4897, 0.4918, 0.5091, 0.5092],\n",
      "         [0.5052, 0.4901, 0.4964, 0.4879],\n",
      "         [0.4902, 0.4987, 0.5067, 0.4994],\n",
      "         [0.5007, 0.4911, 0.4964, 0.4907],\n",
      "         [0.5113, 0.4949, 0.5009, 0.4864],\n",
      "         [0.5129, 0.4886, 0.5105, 0.4875],\n",
      "         [0.4915, 0.5016, 0.5036, 0.5009],\n",
      "         [0.4972, 0.4963, 0.5025, 0.5013],\n",
      "         [0.5020, 0.4839, 0.4927, 0.4906],\n",
      "         [0.4935, 0.4937, 0.5080, 0.4905],\n",
      "         [0.5023, 0.4859, 0.5002, 0.5009],\n",
      "         [0.4962, 0.4908, 0.5069, 0.4960],\n",
      "         [0.5073, 0.4899, 0.4938, 0.4909],\n",
      "         [0.4853, 0.4910, 0.5085, 0.5001],\n",
      "         [0.4847, 0.5002, 0.5037, 0.5079],\n",
      "         [0.4903, 0.4971, 0.5082, 0.4992],\n",
      "         [0.4844, 0.5011, 0.5034, 0.5075],\n",
      "         [0.4961, 0.4955, 0.5073, 0.5016],\n",
      "         [0.4925, 0.4963, 0.5053, 0.4890],\n",
      "         [0.5015, 0.4896, 0.4945, 0.4881],\n",
      "         [0.4948, 0.4919, 0.5073, 0.4916],\n",
      "         [0.5118, 0.4989, 0.5100, 0.4891],\n",
      "         [0.4881, 0.4918, 0.5099, 0.5072],\n",
      "         [0.5064, 0.5024, 0.5081, 0.4913],\n",
      "         [0.4975, 0.4940, 0.4959, 0.4996],\n",
      "         [0.4934, 0.5040, 0.5001, 0.5047],\n",
      "         [0.5086, 0.4973, 0.5132, 0.4996],\n",
      "         [0.4911, 0.4953, 0.5040, 0.4913],\n",
      "         [0.4935, 0.4934, 0.4959, 0.5024],\n",
      "         [0.5016, 0.4894, 0.5033, 0.4935],\n",
      "         [0.4916, 0.5007, 0.5041, 0.5009],\n",
      "         [0.4894, 0.4939, 0.5006, 0.5015],\n",
      "         [0.4897, 0.4958, 0.5070, 0.5107],\n",
      "         [0.4924, 0.4875, 0.5014, 0.4996],\n",
      "         [0.4946, 0.4907, 0.5047, 0.4942],\n",
      "         [0.5026, 0.4889, 0.5021, 0.4917],\n",
      "         [0.4943, 0.4962, 0.5026, 0.4916],\n",
      "         [0.4828, 0.5011, 0.5085, 0.5110],\n",
      "         [0.5052, 0.4884, 0.4991, 0.4887],\n",
      "         [0.5029, 0.4914, 0.4972, 0.4902],\n",
      "         [0.4935, 0.4958, 0.5097, 0.4944],\n",
      "         [0.5004, 0.4909, 0.5022, 0.4941],\n",
      "         [0.4868, 0.4906, 0.5056, 0.5027],\n",
      "         [0.4953, 0.4989, 0.5004, 0.4997],\n",
      "         [0.4799, 0.4997, 0.5080, 0.5029],\n",
      "         [0.4814, 0.4994, 0.5059, 0.5076],\n",
      "         [0.4999, 0.4904, 0.4961, 0.4903],\n",
      "         [0.4920, 0.4947, 0.4983, 0.5025],\n",
      "         [0.5016, 0.4861, 0.4886, 0.4957],\n",
      "         [0.4980, 0.4886, 0.5066, 0.4994],\n",
      "         [0.4898, 0.5001, 0.4966, 0.5021],\n",
      "         [0.5083, 0.4964, 0.5087, 0.4905],\n",
      "         [0.4937, 0.4890, 0.4963, 0.5035],\n",
      "         [0.4972, 0.4866, 0.5082, 0.4995],\n",
      "         [0.5117, 0.4916, 0.5024, 0.4882],\n",
      "         [0.4891, 0.4874, 0.5088, 0.5047],\n",
      "         [0.4971, 0.4918, 0.5045, 0.4907],\n",
      "         [0.5120, 0.4901, 0.5127, 0.4904],\n",
      "         [0.4910, 0.4940, 0.4924, 0.5009],\n",
      "         [0.4908, 0.4975, 0.5073, 0.4995],\n",
      "         [0.4964, 0.4901, 0.4944, 0.4998],\n",
      "         [0.4942, 0.5016, 0.5056, 0.5015],\n",
      "         [0.4953, 0.4956, 0.5090, 0.4973],\n",
      "         [0.4982, 0.4951, 0.5015, 0.5038],\n",
      "         [0.5118, 0.4967, 0.5087, 0.4829],\n",
      "         [0.5001, 0.4971, 0.5065, 0.5048],\n",
      "         [0.5140, 0.4956, 0.5102, 0.4904],\n",
      "         [0.4907, 0.4934, 0.5018, 0.4996],\n",
      "         [0.4957, 0.5012, 0.5010, 0.5048],\n",
      "         [0.5056, 0.4992, 0.5048, 0.4920],\n",
      "         [0.4963, 0.4917, 0.5097, 0.4954],\n",
      "         [0.5002, 0.4932, 0.5090, 0.5013],\n",
      "         [0.4918, 0.5010, 0.4956, 0.5084],\n",
      "         [0.4918, 0.5059, 0.5099, 0.4969],\n",
      "         [0.4962, 0.4997, 0.5037, 0.5008],\n",
      "         [0.4960, 0.4996, 0.5035, 0.5009],\n",
      "         [0.5028, 0.4888, 0.4917, 0.4888],\n",
      "         [0.4913, 0.4962, 0.5001, 0.5018],\n",
      "         [0.5158, 0.4964, 0.5133, 0.4868],\n",
      "         [0.4915, 0.4909, 0.4928, 0.4998],\n",
      "         [0.5020, 0.4884, 0.4967, 0.4908],\n",
      "         [0.5057, 0.5033, 0.5087, 0.4943],\n",
      "         [0.5124, 0.4967, 0.5008, 0.4883],\n",
      "         [0.4968, 0.4945, 0.5040, 0.5017],\n",
      "         [0.4948, 0.4926, 0.5040, 0.5005],\n",
      "         [0.4931, 0.4938, 0.5154, 0.5083],\n",
      "         [0.4963, 0.4968, 0.5039, 0.5030],\n",
      "         [0.4958, 0.4957, 0.5072, 0.4999],\n",
      "         [0.4975, 0.4946, 0.5065, 0.5004],\n",
      "         [0.4926, 0.4904, 0.4959, 0.4967],\n",
      "         [0.5017, 0.4896, 0.4971, 0.4924],\n",
      "         [0.4991, 0.4917, 0.5036, 0.4970],\n",
      "         [0.4936, 0.4935, 0.5042, 0.4970],\n",
      "         [0.5004, 0.4954, 0.4962, 0.4968]]], device='cuda:1',\n",
      "       grad_fn=<SigmoidBackward>)}\n"
     ]
    }
   ],
   "source": [
    "h_lefts = []\n",
    "h_rights = []\n",
    "ys = []\n",
    "for x, y in traingen:\n",
    "    x = img_handler((x,))[0]\n",
    "    h_lefts.append(model.encoder(x[0]))\n",
    "    h_rights.append(model.encoder(x[1]))\n",
    "    ys.append(y)\n",
    "\n",
    "class Gen:\n",
    "    def __init__(self):\n",
    "        self.gen = zip(h_lefts, h_rights, ys)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(ys)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        l, r, y = next(self.gen)\n",
    "        return (l, r), y\n",
    "    \n",
    "model.decoder.train()\n",
    "gener = Gen()\n",
    "for imgs, y in gener:\n",
    "    output = model.decoder(imgs[0], imgs[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "matcher = HungarianMatcher()\n",
    "criterion = SetCriterion(6, matcher, weight_dict, eos_coef = 0.5, losses=losses)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=8e-5, momentum=0.001, dampening=0.000001)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "traingen = TorchStereoDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(8,12))\n",
    "valgen = TorchStereoDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(1024,1024+8))\n",
    "\n",
    "# trainloader = DataLoader(\n",
    "#     dataset=traingen,\n",
    "#     batch_size=1,\n",
    "#     collate_fn=lambda x: tuple(zip(*x)),\n",
    "# )\n",
    "\n",
    "# valloader = DataLoader(\n",
    "#     dataset=valgen,\n",
    "#     batch_size=1,\n",
    "#     collate_fn=lambda x: tuple(zip(*x)),\n",
    "# )\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    dataset=Gener(),\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "valloader = DataLoader(\n",
    "    dataset=valgen,\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "def sanity_dataset(gen: DataLoader, index=0, classmap: Optional[Mapping[int, str]]=None):\n",
    "    geniter = iter(gen)\n",
    "    \n",
    "    for i in range(index+1):\n",
    "        img, targets = next(geniter)\n",
    "    \n",
    "    img = img.numpy().transpose((1,2,0))\n",
    "    boxes = targets['boxes'].numpy()\n",
    "    labels = targets['labels'].numpy()\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for cls, box in zip(labels,boxes):\n",
    "        cxy = box[:2]\n",
    "        # Visualize center xy\n",
    "        ax.add_patch(plt.Circle(cxy*(h, w), 5, facecolor='red', edgecolor='k', alpha=0.4))\n",
    "        \n",
    "        bw, bh = box[2], box[3]\n",
    "        # tlxy = top left xy\n",
    "        tlxy = (cxy - (bw/2, bh/2))*(h, w)\n",
    "        ax.add_patch(plt.Rectangle(tlxy, bw*w, bh*h, fill=False, lw=2, color='red', alpha=0.4))\n",
    "        \n",
    "        try:\n",
    "            ax.text(*tlxy, classmap[int(cls)], fontsize=11, bbox=dict(facecolor='red', alpha=0.5))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n",
    "sanity_dataset(traingen, 16, num2name)\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Epoch 1/20000:   0%|          | 0/4 [00:00<?, ? batches/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-2d51d220008a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-4d0dcc01e1d7>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process interrupted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-4d0dcc01e1d7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(trainloader, valloader, model, criterion, optimizer, n_epochs, device, validate, save_best)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mtrainbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraintqdminfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-4d0dcc01e1d7>\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(context, epoch, n_epochs, leave_tqdm)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Loop through train batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;31m# images = [image.to(device) for image in images]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=20000,\n",
    "    device=device,\n",
    "    save_best=False,\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model.state_dict(), 'overfit_range_8_12_2in1out.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x: torch.Tensor):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def plot_results(img: Image.Image, classes: Iterable, boxes: Iterable, classmap: Optional[Mapping[int, str]]=None, ax: Optional=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16,10))\n",
    "    img = np.array(img)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    if len(boxes) != 0:\n",
    "        h, w = img.shape[:2]\n",
    "        boxes = box_cxcywh_to_xyxy(boxes)\n",
    "        boxes[:,[0,2]] *= w\n",
    "        boxes[:,[1,3]] *= h\n",
    "        \n",
    "        print([int(cls) for cls in classes])\n",
    "        \n",
    "        for cls, (xmin, ymin, xmax, ymax) in zip(classes, boxes):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                       fill=False, color='cyan', linewidth=3))\n",
    "            try:\n",
    "                ax.text(xmin, ymin, classmap[int(cls)], fontsize=11, bbox=dict(facecolor='cyan', alpha=0.9))\n",
    "            except:\n",
    "                ax.text(xmin, ymin, str(int(cls)), fontsize=11, bbox=dict(facecolor='cyan', alpha=0.9))\n",
    "                pass\n",
    "    \n",
    "    if ax is None:\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "    \n",
    "def postprocess(logits: torch.Tensor, boxes: torch.Tensor):\n",
    "#     return logits.argmax(-1), boxes\n",
    "    keepmask = logits.softmax(-1)[:,:-1].max(-1)[0] > 0.2\n",
    "    if any(keepmask) == False:\n",
    "        return torch.Tensor(), torch.Tensor()\n",
    "    return logits[keepmask].argmax(-1), boxes[keepmask]\n",
    "\n",
    "    \n",
    "def eval_model(model, img: torch.Tensor, classmap: Optional[Mapping[int, str]]=None, ax: Optional=None):    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "#         output = model(img.to(device).unsqueeze(0))\n",
    "        output = model((img[0].to(device), img[1].to(device)))\n",
    "        \n",
    "        boxes = output['pred_boxes'][0]\n",
    "        logits = output['pred_logits'][0]\n",
    "        print(logits.argmax(-1))\n",
    "        \n",
    "        logits_, boxes_ = postprocess(logits, boxes)\n",
    "        \n",
    "        plot_results(img[0][0].cpu().numpy().transpose((1,2,0)), logits_, boxes_, classmap, ax=ax)\n",
    "        \n",
    "# x, y = traingen[16]\n",
    "# eval_model(model, x, num2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model2(model: nn.Module, img: Image.Image): \n",
    "    transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    img_torch = transform(img).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        output = model(img_torch.unsqueeze(0))\n",
    "        \n",
    "    boxes = output['pred_boxes'][0]\n",
    "    logits = output['pred_logits'][0]\n",
    "    logits_, boxes_ = postprocess(logits, boxes)\n",
    "    \n",
    "    \n",
    "    plot_results(np.array(img), logits_, boxes_)\n",
    "    \n",
    "def eval_compare_model(model: nn.Module, gen: Iterable, index: int=0, classmap: Optional[Mapping[int, str]]=None):\n",
    "    x, y = gen[index]\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2,figsize=(15,7))\n",
    "    eval_model(model, x, classmap, axes[0])\n",
    "    plot_results(x[0][0].cpu().numpy().transpose((1,2,0)), y['labels'], y['boxes'], classmap, axes[1])\n",
    "    \n",
    "eval_compare_model(model, traingen, index=2, classmap=num2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img = io.imread('test_image2.png').copy() / 255\n",
    "test_img = Image.open('test_image1.jpg').convert('RGB')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_img_torch = transform(test_img)\n",
    "test_img_torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
