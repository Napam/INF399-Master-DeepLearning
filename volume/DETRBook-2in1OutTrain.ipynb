{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skimage import io\n",
    "\n",
    "################# DETR FUCNTIONS FOR LOSS #######################\n",
    "import sys\n",
    "sys.path.append('./detr_custom/')\n",
    "\n",
    "from models.matcher import HungarianMatcher\n",
    "from models.detr import SetCriterion\n",
    "import engine\n",
    "#################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Glob\n",
    "from glob import glob\n",
    "\n",
    "from typing import Iterable, Sequence, List, Tuple, Dict, Optional, Union, Any, Callable, Mapping\n",
    "from types import ModuleType\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from generators import BlenderStandardDataset, TorchStandardDataset\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "import sqlite3 as db\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def pytorch_init_janus_gpu():\n",
    "    device_id = 1\n",
    "    torch.cuda.set_device(device_id)\n",
    "    \n",
    "    # Sanity checks\n",
    "    assert torch.cuda.current_device() == 1, 'Using wrong GPU'\n",
    "    assert torch.cuda.device_count() == 2, 'Cannot find both GPUs'\n",
    "    assert torch.cuda.get_device_name(0) == 'GeForce RTX 2080 Ti', 'Unexpected GPU name'\n",
    "    assert torch.cuda.is_available() == True, 'GPU not available'\n",
    "    return torch.device('cuda', device_id)\n",
    "\n",
    "\n",
    "def reloader(module_or_member: Union[ModuleType, Any]):    \n",
    "    if isinstance(module_or_member, ModuleType):\n",
    "        importlib.reload(module_or_member)\n",
    "        return module\n",
    "    else:\n",
    "        module = importlib.import_module(module_or_member.__module__)\n",
    "        importlib.reload(module)\n",
    "        return module.__dict__[module_or_member.__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1 (GeForce RTX 2080 Ti)\n"
     ]
    }
   ],
   "source": [
    "seed = 42069\n",
    "seed_everything(seed)\n",
    "\n",
    "try:\n",
    "    device = pytorch_init_janus_gpu()\n",
    "    print(f'Using device: {device} ({torch.cuda.get_device_name()})')\n",
    "except AssertionError as e:\n",
    "    print('GPU could not initialize, got error:', e)\n",
    "    device = torch.device('cpu')\n",
    "    print('Device is set to CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_CACHE_DIR = 'torch_cache'\n",
    "DATASET_DIR = '/mnt/blendervol/objdet_std_data'\n",
    "SQL_TABLE = 'bboxes_std'\n",
    "WEIGHTS_DIR = 'fish_statedicts'\n",
    "NUM_CLASSES=6+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con = db.connect(f'file:{os.path.join(DATASET_DIR,\"bboxes.db\")}?mode=ro', uri=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2name = eval(open(os.path.join(DATASET_DIR,\"metadata.txt\"), 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = pd.read_sql_query('SELECT COUNT(DISTINCT(imgnr)) FROM bboxes_std', db_con).values[0][0]\n",
    "\n",
    "TRAIN_RANGE = (0, int(3/4*n_data))\n",
    "VAL_RANGE = (int(3/4*n_data), n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir(TORCH_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  \n",
    "class DETRModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        self.model = torch.hub.load(\n",
    "            os.path.join(TORCH_CACHE_DIR, 'facebookresearch_detr_master'), \n",
    "            model='detr_resnet50', \n",
    "            pretrained=True,\n",
    "            source='local'\n",
    "        )\n",
    "        \n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        \n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)\n",
    "\n",
    "model = DETRModel(NUM_CLASSES, 100)\n",
    "model = model.to(device)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(obj: Any, f: str):\n",
    "    pathlib.Path(f).parent.mkdir(parents=True, exist_ok=True)\n",
    "    assert isinstance(f, str), \"Filename must be of type string when saving model\"\n",
    "    torch.save(obj=obj, f=f)\n",
    "    \n",
    "    \n",
    "def interruptable(f: Callable):\n",
    "    '''Decorator for functions that should handle KeyboardInterrupts more gracefully'''\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            retval = f(*args, **kwargs)\n",
    "        except KeyboardInterrupt as e:\n",
    "            print('Process interrupted')\n",
    "            return \n",
    "        return retval\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def _validate_model(context, traintqdminfo):\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # valbar will disappear after it is done since leave=False\n",
    "    valbar = tqdm(\n",
    "        iterable=enumerate(context['valloader'], 0), \n",
    "        total=len(context['valloader']), \n",
    "        unit=' batches',\n",
    "        desc=f' Validating',\n",
    "        ascii=True,\n",
    "        position=0,\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    model = context['model']\n",
    "    criterion = context['criterion']\n",
    "    \n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    # Loop through val batches\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in valbar:\n",
    "            images = [image.to(device) for image in images]\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            loss_dict = criterion(output, labels)\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "            # print statistics\n",
    "            running_val_loss += losses.item()\n",
    "            val_loss = running_val_loss / (i+1)\n",
    "            valtqdminfo = {**traintqdminfo, 'val loss':val_loss}\n",
    "            valbar.set_postfix(valtqdminfo)\n",
    "            \n",
    "    return valtqdminfo\n",
    "\n",
    "\n",
    "def _train_model(context, epoch, n_epochs, leave_tqdm):\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    trainbar = tqdm(\n",
    "        iterable=enumerate(context['trainloader'], 0),\n",
    "        total=len(context['trainloader']),\n",
    "        unit=' batches',\n",
    "        desc=f' Epoch {epoch+1}/{n_epochs}',\n",
    "        ascii=True,\n",
    "        position=0,\n",
    "        leave=leave_tqdm\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    # Loop through train batches\n",
    "    for i, (images, labels) in trainbar:\n",
    "        images = [image.to(device) for image in images]\n",
    "        labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "        output = model(images)\n",
    "        loss_dict = criterion(output, labels)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # Zero parameter gradients since PyTorch will just accumulate the gradient\n",
    "        # vectors while it trains (in order to get the \"mean\" direction to move in\n",
    "        # the parameter space). Also doing it this way minimizes memory allocation\n",
    "        # etc, probably.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward() # Computes gradients\n",
    "        optimizer.step() # Do a gradient step\n",
    "\n",
    "        # print statistics\n",
    "        running_train_loss += losses.item()\n",
    "        train_loss = running_train_loss / (i+1)\n",
    "        traintqdminfo = {'train loss':train_loss}\n",
    "        trainbar.set_postfix(traintqdminfo)\n",
    "    \n",
    "    return trainbar, traintqdminfo\n",
    "\n",
    "\n",
    "@interruptable\n",
    "def train_model(\n",
    "        trainloader: DataLoader, \n",
    "        valloader: DataLoader, \n",
    "        model: nn.Module, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        n_epochs: int, \n",
    "        device: torch.device, \n",
    "        validate: bool = True,\n",
    "        save_best: bool = True\n",
    "    ):\n",
    "    \n",
    "    # for convenience\n",
    "    context = {\n",
    "        'trainloader':trainloader,\n",
    "        'valloader':valloader,\n",
    "        'model':model,\n",
    "        'criterion':criterion,\n",
    "        'optimizer':optimizer,\n",
    "        'device':device\n",
    "    }\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        trainbar, traintqdminfo = _train_model(context, epoch, n_epochs, not validate)\n",
    "            \n",
    "        if validate:\n",
    "            valtqdminfo = _validate_model(context, traintqdminfo)\n",
    "        \n",
    "            # Extra dirty tqdm hack hehe\n",
    "            # _validate_model will create its own tqdm bar that will replace the bar\n",
    "            # from _train_model, but will clear itself afterwards\n",
    "            # the code below reactivates the previous train bar\n",
    "            trainbar.disable = False\n",
    "            trainbar.set_postfix({**traintqdminfo, **valtqdminfo})\n",
    "            trainbar.disable = True\n",
    "            print(file=sys.stderr)\n",
    "        \n",
    "            # Save best models\n",
    "            if save_best:\n",
    "                if valtqdminfo['val loss'] < best_val_loss:\n",
    "                    best_val_loss = valtqdminfo['val loss']\n",
    "                    isodatenow = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                    daydir = datetime.today().strftime(\"weights_%Y-%m-%d\")\n",
    "                    filename = (\n",
    "                        f'detr_statedicts_epoch{epoch+1}'\n",
    "                        f'_train{traintqdminfo[\"train loss\"]:.4f}_val{best_val_loss:.4f}'\n",
    "                        f'_{isodatenow}.pth'\n",
    "                    )\n",
    "                    filepath = os.path.join(WEIGHTS_DIR, daydir, filename)\n",
    "\n",
    "                    save_model(\n",
    "                        obj={\n",
    "                            'model':model.state_dict(),\n",
    "                            'optimizer':optimizer.state_dict(),\n",
    "                            'criterion':criterion.state_dict(),\n",
    "                        },\n",
    "                        f = filepath\n",
    "                    )\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "from torchvision.models import resnet50\n",
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        \n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.stack(inputs, dim=0).to(device)\n",
    "\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        \n",
    "        print(h.shape)\n",
    "    \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}\n",
    "\n",
    "def prepare_model(model, num_classes):\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\n",
    "        url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "        map_location='cpu',\n",
    "        check_hash=True\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load('fish_statedicts/weights_2021-01-06/detr_statedicts_epoch164_train0.2128_val0.3822_2021-01-06T00:58:28.pth')\n",
    "    model.linear_class = nn.Linear(model.linear_bbox.in_features, num_classes + 1)\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    return model\n",
    "    \n",
    "model = DETRdemo(91)\n",
    "model = prepare_model(model, 6)\n",
    "model = model.to(device)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        \n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "        \n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def get_latent(self, img):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            img = torch.stack(img, dim=0).to(device)\n",
    "\n",
    "        x = self.backbone.conv1(img)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "\n",
    "    \n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        return h\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # (1, 100, 256)\n",
    "        # (batchsize, n_queries, embedding_dim)\n",
    "        h = self.get_latent(imgs)\n",
    "    \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h),\n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}\n",
    "\n",
    "def prepare_model(model, num_classes):\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\n",
    "        url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
    "        map_location='cpu',\n",
    "        check_hash=True\n",
    "    )\n",
    "    \n",
    "#     state_dict = torch.load('fish_statedicts/weights_2021-01-06/detr_statedicts_epoch164_train0.2128_val0.3822_2021-01-06T00:58:28.pth')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.linear_class = nn.Linear(model.linear_bbox.in_features, num_classes + 1)\n",
    "    return model\n",
    "    \n",
    "model = DETRdemo(91)\n",
    "model = prepare_model(model, 6)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {'loss_ce': 2, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "matcher = HungarianMatcher()\n",
    "criterion = SetCriterion(6, matcher, weight_dict, eos_coef = 0.5, losses=losses)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-8)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=8e-5, momentum=0.001, dampening=0.000001)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "traingen = TorchStandardDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(8,12))\n",
    "valgen = TorchStandardDataset(DATASET_DIR, SQL_TABLE, BATCH_SIZE, shuffle=False, imgnrs=range(1024,1024+8))\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    dataset=traingen,\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "valloader = DataLoader(\n",
    "    dataset=valgen,\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "def sanity_dataset(gen: DataLoader, index=0, classmap: Optional[Mapping[int, str]]=None):\n",
    "    geniter = iter(gen)\n",
    "    \n",
    "    for i in range(index+1):\n",
    "        img, targets = next(geniter)\n",
    "    \n",
    "    img = img.numpy().transpose((1,2,0))\n",
    "    boxes = targets['boxes'].numpy()\n",
    "    labels = targets['labels'].numpy()\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for cls, box in zip(labels,boxes):\n",
    "        cxy = box[:2]\n",
    "        # Visualize center xy\n",
    "        ax.add_patch(plt.Circle(cxy*(h, w), 5, facecolor='red', edgecolor='k', alpha=0.4))\n",
    "        \n",
    "        bw, bh = box[2], box[3]\n",
    "        # tlxy = top left xy\n",
    "        tlxy = (cxy - (bw/2, bh/2))*(h, w)\n",
    "        ax.add_patch(plt.Rectangle(tlxy, bw*w, bh*h, fill=False, lw=2, color='red', alpha=0.4))\n",
    "        \n",
    "        try:\n",
    "            ax.text(*tlxy, classmap[int(cls)], fontsize=11, bbox=dict(facecolor='red', alpha=0.5))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n",
    "sanity_dataset(traingen, 16, num2name)\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs=20000,\n",
    "    device=device,\n",
    "    save_best=False,\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model.state_dict(), 'overfit_range_8_12.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x: torch.Tensor):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def plot_results(img: Image.Image, classes: Iterable, boxes: Iterable, classmap: Optional[Mapping[int, str]]=None, ax: Optional=None):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16,10))\n",
    "    img = np.array(img)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    if len(boxes) != 0:\n",
    "        h, w = img.shape[:2]\n",
    "        boxes = box_cxcywh_to_xyxy(boxes)\n",
    "        boxes[:,[0,2]] *= w\n",
    "        boxes[:,[1,3]] *= h\n",
    "        \n",
    "        print([int(cls) for cls in classes])\n",
    "        \n",
    "        for cls, (xmin, ymin, xmax, ymax) in zip(classes, boxes):\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                       fill=False, color='cyan', linewidth=3))\n",
    "            try:\n",
    "                ax.text(xmin, ymin, classmap[int(cls)], fontsize=11, bbox=dict(facecolor='cyan', alpha=0.9))\n",
    "            except:\n",
    "                ax.text(xmin, ymin, str(int(cls)), fontsize=11, bbox=dict(facecolor='cyan', alpha=0.9))\n",
    "                pass\n",
    "    \n",
    "    if ax is None:\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "    \n",
    "def postprocess(logits: torch.Tensor, boxes: torch.Tensor):\n",
    "#     return logits.argmax(-1), boxes\n",
    "    keepmask = logits.softmax(-1)[:,:-1].max(-1)[0] > 0.2\n",
    "    if any(keepmask) == False:\n",
    "        return torch.Tensor(), torch.Tensor()\n",
    "    return logits[keepmask].argmax(-1), boxes[keepmask]\n",
    "\n",
    "    \n",
    "def eval_model(model, img: torch.Tensor, classmap: Optional[Mapping[int, str]]=None, ax: Optional=None):    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        output = model(img.to(device).unsqueeze(0))\n",
    "        \n",
    "        boxes = output['pred_boxes'][0]\n",
    "        logits = output['pred_logits'][0]\n",
    "        print(logits.argmax(-1))\n",
    "        \n",
    "        logits_, boxes_ = postprocess(logits, boxes)\n",
    "        \n",
    "        plot_results(img.cpu().numpy().transpose((1,2,0)), logits_, boxes_, classmap, ax=ax)\n",
    "        \n",
    "# x, y = traingen[16]\n",
    "# eval_model(model, x, num2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model2(model: nn.Module, img: Image.Image): \n",
    "    transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    img_torch = transform(img).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        output = model(img_torch.unsqueeze(0))\n",
    "        \n",
    "    boxes = output['pred_boxes'][0]\n",
    "    logits = output['pred_logits'][0]\n",
    "    logits_, boxes_ = postprocess(logits, boxes)\n",
    "    \n",
    "    \n",
    "    plot_results(np.array(img), logits_, boxes_)\n",
    "    \n",
    "def eval_compare_model(model: nn.Module, gen: Iterable, index: int=0, classmap: Optional[Mapping[int, str]]=None):\n",
    "    x, y = gen[index]\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2,figsize=(15,7))\n",
    "    eval_model(model, x, classmap, axes[0])\n",
    "    plot_results(x.cpu().numpy().transpose((1,2,0)), y['labels'], y['boxes'], classmap, axes[1])\n",
    "    \n",
    "eval_compare_model(model, traingen, index=2, classmap=num2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img = io.imread('test_image2.png').copy() / 255\n",
    "test_img = Image.open('test_image1.jpg').convert('RGB')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_img_torch = transform(test_img)\n",
    "test_img_torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
